### Logistic regression
```{r logistic}
# logistic regression with train dataset

# create train without zipcode
train_df_logis <- train_df_non_linear[,-c(22:91)]
test_df_logis <- test_df_non_linear[,-c(22:91)]

# get median on price

price_med <- median(train_df_logis$price)
price_med
# create categorical response variable. 1=higher than median, 0=lower than median
train_df_logis$price_cat <- 1
train_df_logis[train_df_logis$price < price_med,]$price_cat <- 0
train_df_logis$price_cat <- as.factor(train_df_logis$price_cat)
table(train_df_logis$price_cat)

price_med.test <- median(test_df_non_linear$price)
price_med.test
# create categorical response variable. 1=higher than median, 0=lower than median
test_df_non_linear$price_cat <- 1
test_df_non_linear[test_df_non_linear$price < price_med.test,]$price_cat <- 0
test_df_non_linear$price_cat <- as.factor(test_df_non_linear$price_cat)
table(test_df_non_linear$price_cat)

# logistic regression
lmod <- glm(price_cat ~ . -price, family=binomial, train_df_logis)
summary(lmod)
# beta <- coef(lmod)
# exp(beta)

lmodr <- step(lmod, trace=0)
summary(lmodr)
length(summary(lmodr)$coefficients[,4])

# drop1
drop1(lmodr,test="Chi")
length(summary(lmodr)$coefficients)
#dropping view_2 first (highest pvalue)
lmodr1<-update(lmodr, as.formula(paste(".~.-", "view_2")) )
summary(lmodr1)
length(summary(lmodr1)$coefficients[,4])
#dropping grade_11 next (highest pvalue)
lmodr2<-update(lmodr1, as.formula(paste(".~.-", "grade_11")) )
summary(lmodr2)
length(summary(lmodr2)$coefficients[,4])
#dropping grade_4 next (highest pvalue)
lmodr3<-update(lmodr2, as.formula(paste(".~.-", "grade_4")) )
summary(lmodr3)
length(summary(lmodr3)$coefficients[,4]) # now All variables are significant.

true_labels <- train_df_logis$price_cat
exp<-cbind(true_labels,lmodr3$fitted.values)
exp[1:10,]

#using 0.45 as a cutoff to predict the class. It has the highest F1
predicted_probabilities <- predict(lmodr3, train_df_logis, type="response")
predictions <- ifelse(predicted_probabilities > 0.45, 1, 0)
# Create confusion matrix
confusion_matrix <- caret::confusionMatrix(as.factor(predictions), 
                                    as.factor(true_labels),
                                    mode="prec_recall", positive = "1")

confusion_matrix


# test data 
true_labels.test <- test_df_logis$price_cat
predicted_probabilities.test <- predict(lmodr3, test_df_logis, type="response")
predictions.test <- ifelse(predicted_probabilities.test > 0.45, 1, 0)
# Create confusion matrix
confusion_matrix.test <- caret::confusionMatrix(as.factor(predictions.test), 
                                    as.factor(true_labels.test),
                                    mode="prec_recall", positive = "1")

confusion_matrix.test

confusion_matrix.output <- as.data.frame(rbind(c("train",round(confusion_matrix$byClass[[1]],3),
                                    round(confusion_matrix$byClass[[2]],3),
                                           round(confusion_matrix$byClass[[5]],3),
                                           round(confusion_matrix$byClass[[7]],3)),
                                         c("test",round(confusion_matrix.test$byClass[[1]],3),
                                           round(confusion_matrix.test$byClass[[2]],3),
                                           round(confusion_matrix.test$byClass[[5]],3),
                                           round(confusion_matrix.test$byClass[[7]],3))) )      
colnames(confusion_matrix.output) <- c("data","Sensitivity","Specificity","Precision","F1")
confusion_matrix.output
```