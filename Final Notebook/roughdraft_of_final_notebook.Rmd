---
title: "EXT CSCI E-106 Model Data Class Group Project Template"
author: "Charles Atchison"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: true
    css: "extra_html_files/style.css"
    includes:
      before_body: extra_html_files/header.html
      after_body: extra_html_files/copyright.html
---

```{r setup, include=FALSE}
# List of required packages, including 'here'
required_packages <- c(
  "plyr", "alr4", "caret", "car", "corrplot", "dplyr", "effects", "fastDummies", "ggplot2",
  "GGally", "ggplot2", "ggpubr", "glmnet", "lmtest", "MASS", "ModelMetrics", "kableExtra",
  "nortest", "olsrr", "onewaytests", "readr", "here", "stringr", "knitr", "reshape2", "leaflet",
  "RColorBrewer", "scales", "purrr", "DT", "jsonlite", "magrittr", "rpart", "broom"
)

# Establish CRAN for package installs
options(repos = c(CRAN = "https://ftp.osuosl.org/pub/cran/")) # Set the CRAN mirror

# Check if each package is installed; if not, install it
for (pkg in required_packages) {
  if (!(pkg %in% installed.packages()[,"Package"])) {
    install.packages(pkg, dependencies = TRUE)
  }
}

# Load all the packages
lapply(required_packages, library, character.only = TRUE)

# Build the full path to the directory containing the Rmd file
rmd_dir <- dirname(here())

# Navigate up one directory and then to the CSV data file
csv_file <- file.path(rmd_dir, "HomeValueForecaster", "KC_House_Sales.csv")

json_filepath <- file.path(rmd_dir, "HomeValueForecaster/Final Notebook", "model_parameters.json")

# Read the CSV file into a data frame
df <- read.csv(csv_file)

# Function to manually update the JSON file with model parameters
update_model_json <- function(model_name, features, filepath) {
  # Read existing parameters if the file exists, or initialize an empty list
  model_params <- if (file.exists(filepath)) {
                    fromJSON(filepath)
                  } else {
                    list()
                  }
  # Update the parameters for the specified model
  model_params[[model_name]] <- features
  # Write the updated parameters back to the JSON file
  write_json(model_params, filepath)
}

# Initialize the figure counter
fig_counter <- 0

# Custom function to generate figure captions with automatic numbering
generate_figure_caption <- function(caption) {
  fig_counter <<- fig_counter + 1
  paste0("Figure 1.", fig_counter, " ", caption)
}
```


## I. Introduction: Contextualizing the King County House Price Prediction Model

### Problem Statement and Objective
In the vibrant and diverse King County real estate market, including Seattle's dynamic environment, property prices are shaped by an array of variables. The primary challenge is to construct a predictive model that can accurately estimate house prices within this area. Utilizing a comprehensive dataset that encompasses diverse house attributes, this model aims to decode the complex mechanisms influencing house pricing.

### Purpose of the Model

#### <ins>Predictive Accuracy</ins>
The model strives to offer precise price predictions for properties in King County by effectively correlating various house features with their market prices. This aspect is crucial in understanding and quantifying how different characteristics impact the value of a property.

#### <ins>Analytical Insight</ins>
A key goal of the model is to unearth and interpret the multitude of factors that play a significant role in determining house prices within the region. This venture goes beyond mere statistical analysis to provide practical, real-world insights, thereby enriching the understanding of real estate dynamics for all stakeholders.

#### <ins>Decision Support</ins>
The model is designed to be a powerful asset for a range of users, including real estate agents, prospective buyers, and sellers. By offering accurate price predictions and deep market insights, it aids in making informed and strategic decisions in the property market.

### Scope and Methodology

#### <ins>Data Preprocessing and Exploration</ins>
Initial data preparation is vital to ensure accuracy in the model. This stage involves cleansing the data, converting data types, and creating dummy variables for categorical features. Following this, an exploratory data analysis (EDA) is conducted to delve into the dataset's characteristics, examining statistical summaries and relationships between variables.

#### <ins>Feature Selection and Model Assumptions</ins>
The process involves using statistical techniques like stepwise regression for feature selection and conducting tests like the Variable Inflation Factor (VIF) and Anderson-Darling to check for multicollinearity and normality, respectively. Additionally, diagnostic plots are used for detecting outliers.

#### <ins>Model Development and Validation</ins>
A range of models are employed and assessed:

  <ins>**Linear Models:**</ins> Including Ordinary Least Squares (OLS) and Weighted Least Squares (WLS).

  <ins>**Regularization Techniques:**</ins> Such as Ridge, Lasso, and Elastic Net to handle multicollinearity.

  <ins>**Robust Regression:**</ins> Utilizing Huber’s method to minimize the influence of outliers.

  <ins>**Advanced Models:**</ins> Exploring alternatives like regression trees, neural networks (NN), or support vector machines (SVM).

#### <ins>Model Performance Evaluation</ins>
The model's effectiveness is evaluated using metrics like RMSE and R-squared, across both the training (70%) and testing (30%) data sets, to ensure its reliability and applicability in real-world scenarios.

### Conclusion
This introduction sets the stage for a comprehensive analysis, highlighting the multifaceted approach adopted in this project. From meticulous data preparation to sophisticated modeling, the endeavor is not just to predict house prices accurately but also to provide valuable insights into King County's real estate market.


## II. Description of the Data and Quality

### Dataset Overview and Detailed Description
The King County house sales dataset is a comprehensive collection of 21,613 observations, each representing a unique house sale. The dataset encompasses a variety of features that describe different aspects of the houses sold. Below is a detailed description of each variable in the dataset:

```{r data_description, echo = FALSE, include = TRUE}
# Create a data frame for the table
data_description <- data.frame(
  Variable = c(
    'id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',
    'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',
    'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat',
    'long', 'sqft_living15', 'sqft_lot15'
  ),
  Description = c(
    'Unique ID for each home sold (not used as a predictor)',
    'Date of the home sale',
    'Price of each home sold',
    'Number of bedrooms',
    'Number of bathrooms, ".5" accounts for a bathroom with a toilet but no shower',
    'Square footage of the apartment interior living space',
    'Square footage of the land space',
    'Number of floors',
    'A dummy variable for whether the apartment was overlooking the waterfront or not',
    'An index from 0 to 4 of how good the view of the property was',
    'An index from 1 to 5 on the condition of the apartment',
    'An index from 1 to 13 about building construction and design quality',
    'The square footage of the interior housing space above ground level',
    'The square footage of the interior housing space below ground level',
    'The year the house was initially built',
    'The year of the house’s last renovation',
    'The zipcode area the house is in',
    'Latitude coordinate',
    'Longitude coordinate',
    'The square footage of interior housing living space for the nearest 15 neighbors',
    'The square footage of the land lots of the nearest 15 neighbors'
  )
)

# Create the table with kable
data_description_table <- kable(
  data_description,
  format = "html",
  caption = generate_figure_caption(caption = "Data Description")
) %>%
  kable_styling(full_width = TRUE) %>%
  column_spec(1, bold = TRUE)

# Print the table
data_description_table
```

### Data Quality and Transformation

#### <ins>Data Cleaning and Transformation</ins>
The dataset's preparation involved meticulous cleaning and transformation processes to optimize it for accurate predictive analysis. Key steps undertaken include:

1. **Exclusion of Non-Predictive Variables**:
   - The `id` variable, representing a unique identifier for each house sale, does not contribute to predicting house prices and was therefore removed. This step is crucial in focusing the model on variables that influence the outcome (price).
   - Unlike other non-predictive variables, `lat` (latitude) and `long` (longitude) were initially retained for their crucial role in calculating geographical distances, which could potentially influence house prices.

2. **Transformation of Data Types**:
   - The `date` variable, initially in a string format, was transformed into a numeric format. This conversion is essential for incorporating the date into statistical models, as numeric representations are more amenable to various types of analysis.
   - For variables like `price`, `sqft_living`, `sqft_lot`, etc., necessary conversions were performed to ensure they are in a suitable numeric format.

3. **Creation of Dummy Variables for Categorical Data**:
   - Categorical variables like `waterfront`, `view`, `condition`, and `grade` were transformed into dummy variables. This transformation is pivotal for regression analysis as it allows these non-numeric variables to be effectively included in the model.
   - The process involved converting these categorical variables into a series of binary variables (0 or 1). This is particularly important for variables like `waterfront`, which is a binary indicator itself, and for ordinal variables like `view` and `condition`, which have intrinsic order but need to be numerically represented for modeling.

4. **Handling Special Cases in Variables**:
   - For variables like `bathrooms`, where values like "0.5" represent bathrooms with a toilet but no shower, the data was kept as is, considering these nuances convey important information about the house's characteristics.

5. **Grouping and Clustering of Variables**:
   - The `zipcode` variable was transformed by extracting the first three digits, which helps in reducing the number of dummy variables and preventing the model from becoming overly complex while still capturing the geographical influences on house prices.
   - The `grade` variable was clustered into broader categories to simplify the model and focus on significant differences in construction and design quality.

6. **Haversine Distance Calculation**:
   - To incorporate the influence of location more precisely, the Haversine distance was calculated. This involved creating a function to calculate the distance between two geographical points (latitude and longitude) and applying this to our dataset.
   - The calculation of `haversine_distance` is particularly significant for understanding the spatial relationships and proximity to key locations that might affect house prices.

7. **Calculation of Convergence Point**:
   - The dataset was used to identify a 'convergence point' – a central point derived from houses with the highest values. This point served as a reference to calculate each property's distance from a high-value central location, possibly a marker of a desirable area.
   - This step was critical in ensuring that the model accounts for locational desirability without causing data leakage, as it was based solely on the training set.

```{r data_transformation}
# Data Preprocessing and Transformation
set.seed(1023)  # Setting a seed for reproducibility
split_index <- sample(1:nrow(df), size = 0.7 * nrow(df))
train_df <- df[split_index, ]
test_df <- df[-split_index, ]

# Remove non-numeric characters from the 'price' column and convert it to numeric
df$price <- as.numeric(str_replace_all(df$price, "[^0-9.]", ""))

# Calculation of Convergence Point: Determine the convergence point for high-value homes
high_value_threshold <- quantile(df$price, probs = 0.95, na.rm = TRUE)  # Calculate the high-value threshold
high_value_homes <- df[df$price >= high_value_threshold, ]  # Select high-value homes
convergence_point <- c(mean(high_value_homes$lat, na.rm = TRUE), mean(high_value_homes$long, na.rm = TRUE))

# Remove non-numeric characters from the 'price' column and convert it to numeric
train_df$price <- as.numeric(str_replace_all(train_df$price, "[^0-9.]", ""))
test_df$price <- as.numeric(str_replace_all(test_df$price, "[^0-9.]", ""))

# Data Transformation Function with Distance Binning Option
transform_data <- function(df, convergence_point, linear_model) {
  # Date Transformation: Convert the 'date' column to a Date object if present
  if ("date" %in% colnames(df)) {
    df$date <- as.Date(substr(as.character(df$date), 1, 8), format="%Y%m%d")
    # Date-Time Feature Engineering: Extract various date-related features
    df$year_sold <- lubridate::year(df$date)
    df$month_sold <- lubridate::month(df$date)
    df$day_sold <- lubridate::day(df$date)
    df$season <- factor(lubridate::quarter(df$date), labels = c("Winter", "Spring", "Summer", "Fall"))
    df$week_of_year <- lubridate::week(df$date)
    df$day_of_year <- lubridate::yday(df$date)
  }
  # Creating Dummy Variables: Convert categorical variables into dummy variables
  df <- df %>%
    mutate(zipcode = as.factor(zipcode),
           waterfront = as.factor(waterfront),
           view = as.factor(view),
           condition = as.factor(condition),
           grade = as.numeric(grade),
           grade = case_when(
             grade %in% 1:3 ~ "Below_Average",
             grade %in% 4:10 ~ "Average",
             grade %in% 11:13 ~ "Above_Average")) %>%
    dummy_cols(select_columns = c('zipcode', 'view', 'condition', 'grade', 'waterfront', 'season'))
  # Remove last dummy variables to avoid multicollinearity
  if (linear_model) {
    df <- df[, !(names(df) %in% c("zipcode_98199", "view_0", "condition_1", "grade_13", "season_Winter", "waterfront_1"))]
  }
  # Haversine Distance Function: Calculate the distance between two points on Earth's surface
  haversine_distance <- function(lat1, long1, lat2, long2) {
    R <- 6371  # Earth radius in kilometers
    delta_lat <- (lat2 - lat1) * pi / 180
    delta_long <- (long2 - long1) * pi / 180
    a <- sin(delta_lat/2)^2 + cos(lat1 * pi / 180) * cos(lat2 * pi / 180) * sin(delta_long/2)^2
    c <- 2 * atan2(sqrt(a), sqrt(1 - a))
    d <- R * c  # Calculate the haversine distance
    return(d)
  }
  # Calculate Haversine Distance
  df$distance_to_convergence <- mapply(haversine_distance, df$lat, df$long,
                                       MoreArgs = list(lat2 = convergence_point[1], long2 = convergence_point[2]))
  # Remove columns that are no longer needed
  df <- df[, !(names(df) %in% c("id", "date", "zipcode", "view", "condition", "grade", "waterfront", "season"))]
  return(df)
}
# Applying the transformation function to training and test sets
train_df_linear <- transform_data(train_df, convergence_point, linear_model = TRUE)  # Transform the training data for linear models
test_df_linear <- transform_data(test_df, convergence_point, linear_model = TRUE)    # Transform the test data for linear models
train_df_non_linear <- transform_data(train_df, convergence_point, linear_model = FALSE)  # Transform the training data
test_df_non_linear <- transform_data(test_df, convergence_point, linear_model = FALSE)    # Transform the test data

# Set this to TRUE to update all the json model_parameters that are stored the JSON
# Check if the update_model_parameters is TRUE or not
update_model_parameters <- FALSE

# This updates the json with the parameters that were obtained from the intensive process of running
update_model_json <- function(model_name, features, filepath) {
  model_params <- if (file.exists(filepath)) {
                    fromJSON(filepath)
                  } else {
                    list()
                  }
  model_params[[model_name]] <- features
  write_json(model_params, filepath)
}

# # Save the transformed data as CSV files in the current directory
# write.csv(train_df_linear, "train_df_linear.csv", row.names = FALSE)
# write.csv(test_df_linear, "test_df_linear.csv", row.names = FALSE)
# write.csv(train_df_non_linear, "train_df_non_linear.csv", row.names = FALSE)
# write.csv(test_df_non_linear, "test_df_non_linear.csv", row.names = FALSE)
```

#### Training Data Header
```{r view_head_data, echo = FALSE, include = TRUE, results = 'asis'}
# Specify the height for the scrollable div
table_height <- "400px"

# Create a scrollable div and place the table inside it
cat(sprintf('<div style="overflow-y: scroll; max-height: %s;">', table_height))
# Print the table with a caption using the custom function
kable(
  head(train_df_linear),
  caption = generate_figure_caption("Data Header"),
  format = "html"
)
cat('</div>')
```

### Statistical Analysis and Correlation

#### <ins>Exploratory Data Analysis (EDA)</ins>
The exploratory data analysis (EDA) conducted on the King County house sales dataset is an in-depth exploration aimed at uncovering patterns, anomalies, and relationships within the data. This comprehensive EDA includes a variety of analyses to gain a holistic understanding of the dataset's characteristics.

1. **Distribution Analysis of Continuous Variables**:
   - This analysis focuses on continuous variables like `price`, `sqft_living`, `sqft_lot`, and others. Key aspects include examining their distributions, identifying potential outliers, and understanding their range and central tendencies.
   - Histograms and box plots are used to visualize these distributions, which can reveal skewness, kurtosis, and other distributional characteristics important for model assumptions.

2. **Categorical Variable Analysis**:
   - The distribution and count of categorical variables such as `bedrooms`, `bathrooms`, `floors`, `waterfront`, `view`, `condition`, and `grade` are analyzed.
   - Bar plots and frequency tables help in understanding the prevalence of different categories and their potential impact on house prices.

3. **Correlation Analysis**:
   - Understanding how continuous variables correlate with each other and, more importantly, with the target variable `price`.
   - A correlation matrix and corresponding heat map provide a visual and quantitative view of these relationships, highlighting variables that might have a strong positive or negative relationship with house prices.

4. **Temporal Trends Analysis**:
   - Analyzing the influence of time-related features such as `year_sold`, `month_sold`, and `season` on house prices.
   - Time series plots and seasonal decomposition can reveal trends, seasonality, and cyclical patterns in house prices.

5. **Geographical Influence Analysis**:
   - Investigating the spatial aspect by analyzing the `distance_to_convergence` variable.
   - Scatter plots or spatial heat maps can illustrate if proximity to the high-value convergence point influences house prices.


### Continuous Variable Analysis

This analysis focuses on continuous variables like `price`, `sqft_living`, `sqft_lot`, and others. Key aspects include examining their distributions, identifying potential outliers, and understanding their range and central tendencies.

#### Price vs. Square Footage of Living Space

```{r price_vs_sqft_living, echo = FALSE, include = TRUE}
# Scatter plot of Price vs. Square Footage of Living Space
ggplot(data = train_df_non_linear, aes(x = sqft_living, y = price)) +
  geom_point(pch = 20, col = "blue") +
  labs(title = "Price vs. Square Footage of Living Space",
       subtitle = "Seattle Housing Data",
       x = "Sqft Living Space",
       y = "Price",
       caption = generate_figure_caption("Price vs. Square Footage of Living Space"))
```

In the scatter plot above, we compare the `price` of homes against their `sqft_living` (square footage of interior living space). This visualization allows us to explore the relationship between these two variables.

```{r distribution_sqft_living, echo = FALSE, include = TRUE}
# Distribution of Square Footage of Living Space
ggplot(data = train_df_non_linear, aes(x = sqft_living)) +
  geom_histogram(bins = 50) +
  labs(title = "Distribution of Sqft Living Space",
       x = "Sqft Living Space",
       y = "Density",
       caption = generate_figure_caption("Distribution of Sqft Living Space"))
```

The histogram above displays the distribution of `sqft_living`. It reveals that the variable is right-skewed, with most homes having smaller living spaces and relatively fewer very large living spaces.

#### Price vs. Square Footage of Lot

```{r price_vs_sqft_lot, echo = FALSE, include = TRUE}
# Scatter plot of Price vs. Square Footage of Lot
ggplot(data = train_df_non_linear, aes(x = sqft_lot, y = price)) +
  geom_point(pch = 20, col = "green") +
  labs(title = "Price vs. Sqft Lot",
       x = "Sqft Lot",
       y = "Price",
       caption = generate_figure_caption("Price vs. Sqft Lot"))
```

The scatter plot above compares `price` against `sqft_lot` (square footage of land space). It helps us understand if there's any relationship between the size of the lot and the sale price.

```{r distribution_sqft_lot, echo = FALSE, include = TRUE}
# Distribution of Square Footage of Lot
ggplot(data = train_df_non_linear, aes(x = sqft_lot)) +
  geom_histogram(bins = 50) +
  labs(title = "Distribution of Sqft Lot",
       x = "Sqft Lot",
       y = "Density",
       caption = generate_figure_caption("Distribution of Sqft Lot"))
```

The histogram above visualizes the distribution of `sqft_lot`. Similar to `sqft_living`, this variable is right-skewed, with most homes having smaller lot sizes and relatively fewer very large lots.

#### Price vs. Square Footage Above Ground

```{r price_vs_sqft_above, echo = FALSE, include = TRUE}
# Scatter plot of Price vs. Square Footage Above Ground
ggplot(data = train_df_non_linear, aes(x = sqft_above, y = price)) +
  geom_point(pch = 20, col = "red") +
  labs(title = "Price vs. Sqft Above Ground",
       x = "Sqft Above Ground",
       y = "Price",
       caption = generate_figure_caption("Price vs. Sqft Above Ground"))
```

In the scatter plot above, we compare `price` against `sqft_above` (square footage of the interior housing space above ground level). This analysis helps us explore the impact of above-ground living space on home prices.

```{r distribution_sqft_above, echo = FALSE, include = TRUE}
# Distribution of Square Footage Above Ground
ggplot(data = train_df_non_linear, aes(x = sqft_above)) +
  geom_histogram(bins = 50) +
  labs(title = "Distribution of Sqft Above Ground",
       x = "Sqft Above Ground",
       y = "Density",
       caption = generate_figure_caption("Distribution of Sqft Above Ground"))
```

The histogram above shows the distribution of `sqft_above`. It suggests that most homes have similar above-ground square footage, with relatively fewer having significantly larger or smaller above-ground spaces.

#### Price vs. Square Footage of Basement

Excluding homes that do not have a basement.

```{r price_vs_sqft_basement, echo = FALSE, include = TRUE}
# Scatter plot of Price vs. Square Footage of Basement (excluding 0 sqft basement)
# Filter data for non-zero sqft_basement
filtered_data <- train_df_non_linear[train_df_non_linear$sqft_basement > 0,]

# Scatter plot with custom caption
ggplot(data = filtered_data, aes(x = sqft_basement, y = price)) +
  geom_point(pch = 20, col = "purple") +
  labs(title = "Price vs. Sqft Basement",
       x = "Sqft Basement",
       y = "Price",
       caption = generate_figure_caption("Price vs. Sqft Basement (Non-Zero Values)"))
```

The scatter plot above compares `price` against `sqft_basement` (square footage of the interior housing space below ground level). This visualization helps us understand if the presence and size of a basement influence home prices.

```{r distribution_sqft_basement, echo = FALSE, include = TRUE}
# Distribution of Square Footage of Basement (excluding 0 values)
# Histogram with custom caption
ggplot(data = filtered_data, aes(x = sqft_basement)) +
  geom_histogram(bins = 50) +
  labs(title = "Distribution of Sqft Basement",
       x = "Sqft Basement",
       y = "Density",
       caption = generate_figure_caption("Distribution of Sqft Basement (Non-Zero Values)"))
```

The histogram above visualizes the distribution of `sqft_basement`. It indicates that most homes have little to no basement space, while some have larger basement areas.

#### Price vs. Year Built

```{r price_vs_yr_built, echo = FALSE, include = TRUE}
# Scatter plot of Price vs. Year Built
ggplot(data = train_df_non_linear, aes(x = yr_built, y = price)) +
  geom_point(pch = 20, col = "orange") +
  labs(title = "Price vs. Year Built",
       x = "Year Built",
       y = "Price",
       caption = generate_figure_caption("Price vs. Year Built"))
```

The scatter plot above compares `price` against the year when homes were initially built (`yr_built`). This analysis helps us understand how the age of a home relates to its sale price.

```{r distribution_yr_built, echo = FALSE, include = TRUE}
# Distribution of Year Built
ggplot(data = train_df_non_linear, aes(x = yr_built)) +
  geom_histogram(bins = 50) +
  labs(title = "Distribution of Year Built",
       x = "Year Built",
       y = "Density",
       caption = generate_figure_caption("Distribution of Year Built"))
```

The histogram above displays the distribution of `yr_built`. It provides insights into the distribution of home ages in the dataset.

#### Price vs. Year of Last Renovation

Excluding homes that did not have a documented renovation.

```{r price_vs_yr_renovated, echo = FALSE, include = TRUE}
# Find lowest non-zero year renovated
lowest_non_zero_renovation_year <- min(train_df_non_linear$yr_renovated[train_df_non_linear$yr_renovated > 0]) - 1

# Filter data for non-zero yr_renovated
filtered_data <- train_df_non_linear[train_df_non_linear$yr_renovated > 0,]

# Scatter plot of Price vs. Year Renovated
lowest_non_zero_renovation_year <- min(train_df_non_linear$yr_renovated[train_df_non_linear$yr_renovated > 0]) - 1
ggplot(data = filtered_data, aes(x = yr_renovated, y = price)) +
  geom_point(pch = 20, col = "brown") +
  labs(title = "Price vs. Year Renovated",
       x = "Year Renovated",
       y = "Price",
       caption = generate_figure_caption("Price vs. Year Renovated (Non-Zero Values)")) +
  xlim(c(lowest_non_zero_renovation_year, max(train_df_non_linear$yr_renovated)))
```

In the scatter plot above, we compare `price` against the year of the last renovation (`yr_renovated`). This analysis helps us understand whether recent renovations impact home prices.

```{r distribution_yr_renovated, echo = FALSE, include = TRUE, warning = FALSE}
# Find lowest non-zero year renovated
lowest_non_zero_renovation_year <- min(train_df_non_linear$yr_renovated[train_df_non_linear$yr_renovated > 0]) - 1

# Filter data for non-zero yr_renovated
filtered_data <- train_df_non_linear[train_df_non_linear$yr_renovated > 0,]

# Histogram of Year Renovated
ggplot(data = filtered_data, aes(x = yr_renovated)) +
  geom_histogram(fill = "orange") +
  labs(title = "Histogram of Year Renovated",
       x = "Year Renovated",
       y = "Density",
       caption = generate_figure_caption("Histogram of Year Renovated (Non-Zero Values)")) +
  xlim(c(lowest_non_zero_renovation_year, max(train_df_non_linear$yr_renovated)))
```

The histogram above visualizes the distribution of `yr_renovated`. It provides insights into the distribution of renovation years in the dataset.

#### Price vs. Distance to Convergence

```{r price_vs_distance_to_convergence, echo = FALSE, include = TRUE}
# Scatter plot of Price vs. Distance to Convergence
ggplot(data = train_df_non_linear, aes(x = distance_to_convergence, y = price)) +
  geom_point(pch = 20, col = "violet") +
  labs(title = "Price vs. Distance to Convergence",
       x = "Distance to Convergence",
       y = "Price",
       caption = generate_figure_caption("Price vs. Distance to Convergence"))
```

The scatter plot above compares `price` against `distance_to_convergence`. This analysis helps us explore whether the distance to a convergence point impacts home prices.

```{r distribution_distance_to_convergence, echo = FALSE, include = TRUE}
# Distribution of Distance to Convergence
ggplot(data = train_df_non_linear, aes(x = distance_to_convergence)) +
  geom_histogram(bins = 50) +
  labs(title = "Distribution of Distance to Convergence",
       x = "Distance to Convergence",
       y = "Density",
       caption = generate_figure_caption("Distribution of Distance to Convergence"))
```

### Categorical Variable Analysis

The distribution and count of categorical variables such as `bedrooms`, `bathrooms`, `floors`, `waterfront`, `view`, `condition`, and `grade` are analyzed.

#### Price vs. Bedrooms

```{r price_vs_bedrooms, echo = FALSE, include = TRUE}
# Convert bedrooms to factor
train_df_non_linear$bedrooms_factor <- factor(train_df_non_linear$bedrooms)

# Binned Boxplot of Price vs. Bedrooms
ggplot(data = train_df_non_linear, aes(x = bedrooms_factor, y = price)) +
  geom_boxplot(fill = "blue") +
  labs(title = "Price vs. Bedrooms",
       x = "Bedrooms",
       y = "Price",
       caption = generate_figure_caption("Price vs. Bedrooms"))
```

The scatter plot above compares `price` against the number of `bedrooms`. This visualization helps us understand how the number of bedrooms influences home prices.

```{r distribution_bedrooms, echo = FALSE, include = TRUE}
# Filter data excluding 33 bedrooms
filtered_bedrooms <- train_df_non_linear$bedrooms[train_df_non_linear$bedrooms != 33]

# Calculate frequencies of each bedroom count
bedroom_frequencies <- table(filtered_bedrooms)

ggplot(data = data.frame(filtered_bedrooms = as.factor(names(bedroom_frequencies)),
                        filtered_counts = as.numeric(bedroom_frequencies)),
       aes(x = filtered_bedrooms, y = filtered_counts)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(title = "Distribution of Bedrooms (Excluding 33 Bedrooms)",
       x = "Number of Bedrooms",
       y = "Frequency",
       caption = generate_figure_caption("Distribution of Bedrooms (Excluding 33 Bedrooms)"))
```

The bar plot above displays the distribution of the `bedrooms` variable, showing the frequency of each bedroom count.

#### Price vs. Bathrooms

```{r price_vs_bathrooms, echo = FALSE, include = TRUE}
# Convert bathrooms to factor
train_df_non_linear$bathrooms_factor <- factor(train_df_non_linear$bathrooms)

# Binned Boxplot of Price vs. Bathrooms
ggplot(data = train_df_non_linear, aes(x = bathrooms_factor, y = price)) +
  geom_boxplot(fill = "green") +
  labs(title = "Price vs. Bathrooms",
       x = "Bathrooms",
       y = "Price",
       caption = generate_figure_caption("Price vs. Bathrooms"))
```

In the scatter plot above, we compare `price` against the number of `bathrooms`. This analysis helps us explore the relationship between the number of bathrooms and home prices.

```{r distribution_bathrooms, echo = FALSE, include = TRUE}
# Get data for bar plot
bathrooms_counts <- table(train_df_non_linear$bathrooms)
bathrooms <- as.numeric(names(bathrooms_counts))
counts <- as.numeric(bathrooms_counts)

# Bar plot for the distribution of Bathrooms
ggplot(data = data.frame(bathrooms, counts), aes(x = bathrooms, y = counts)) +
  geom_bar(stat = "identity", fill = "green") +
  labs(title = "Distribution of Bathrooms",
       x = "Number of Bathrooms",
       y = "Frequency",
       caption = generate_figure_caption("Distribution of Bathrooms"))
```

The bar plot above visualizes the distribution of the `bathrooms` variable, showing the frequency of each bathroom count.

#### Price vs. Floors

```{r price_vs_floors, echo = FALSE, include = TRUE}
# Binned Boxplot of Price vs. Floors
ggplot(data = train_df_non_linear, aes(x = floors, y = price, group = floors)) +
  geom_boxplot(fill = "orange") +
  labs(title = "Price vs. Floors",
       x = "Floors",
       y = "Price",
       caption = generate_figure_caption("Price vs. Floors"))
```

The scatter plot above compares `price` against the number of `floors`. This analysis helps us understand how the number of floors in a home relates to its sale price.

```{r distribution_floors, echo = FALSE, include = TRUE}
floors_counts <- table(train_df_non_linear$floors)
floors <- as.numeric(names(floors_counts))
counts <- as.numeric(floors_counts)

# Bar plot for the distribution of Floors
ggplot(data = data.frame(floors, counts), aes(x = floors, y = counts)) +
  geom_bar(stat = "identity", fill = "orange") +
  labs(title = "Distribution of Floors",
       x = "Number of Floors",
       y = "Frequency",
       caption = generate_figure_caption("Distribution of Floors"))
```

The bar plot above displays the distribution of the `floors` variable, showing the frequency of each floor count.

#### Price vs. Waterfront

```{r price_vs_waterfront, echo = FALSE, include = TRUE}
ggplot(data = train_df_non_linear, aes(x = waterfront_1, y = price, group = waterfront_1)) +
  geom_boxplot(fill = "purple") +
  labs(title = "Price vs. Waterfront",
       x = "Waterfront",
       y = "Price",
       caption = generate_figure_caption("Price vs. Waterfront"),
       fill = "Waterfront",
       levels = c("No", "Yes"))  # Labels for waterfront status
```

In the scatter plot above, we compare `price` against the `waterfront` variable. This visualization helps us explore how having a waterfront view impacts home prices.

```{r distribution_waterfront, echo = FALSE, include = TRUE}
# Get data for bar plot
waterfront_counts <- table(train_df_non_linear$waterfront_1)
waterfront <- as.numeric(names(waterfront_counts))
counts <- as.numeric(waterfront_counts)

# Bar plot for the distribution of Waterfront
ggplot(data = data.frame(waterfront, counts), aes(x = waterfront, y = counts)) +
  geom_bar(stat = "identity", fill = "purple") +
  labs(title = "Distribution of Waterfront",
       x = "Waterfront",
       y = "Frequency",
       caption = generate_figure_caption("Distribution of Waterfront"),
       fill = "Waterfront",
       levels = c("No", "Yes"))  # Labels for waterfront status

```

The bar plot above visualizes the distribution of the `waterfront` variable, showing the frequency of waterfront and non-waterfront properties.

#### Price vs. View

```{r price_vs_view, echo = FALSE, include = TRUE}
# Convert view categories from dummy variables to a factor for better labeling in ggplot
train_df_non_linear$view_category <- factor(apply(train_df_non_linear[, c("view_0", "view_1", "view_2", "view_3", "view_4")], 1, function(x) which(x == 1)),
                                             labels = c("View 0", "View 1", "View 2", "View 3", "View 4"))

# Create the boxplot with ggplot2
ggplot(train_df_non_linear, aes(x = view_category, y = price)) +
  geom_boxplot(fill = "brown") +
  labs(title = "Price vs. View Quality",
       x = "View Quality",
       y = "Price",
       caption = generate_figure_caption("Boxplot of Price vs. View Quality"))
```

The scatter plot above compares `price` against the `view` variable, which represents the quality of the property's view. This analysis helps us explore how the view quality impacts home prices.

```{r distribution_view, echo = FALSE, include = TRUE}
# Calculate frequencies of each view quality rating
view_frequencies <- colSums(train_df_non_linear[, c("view_0", "view_1", "view_2", "view_3", "view_4")])

# Convert frequencies to data frame for ggplot2
view_df <- data.frame(View = names(view_frequencies), Frequency = view_frequencies)

# Create the bar plot with ggplot2
ggplot(view_df, aes(x = View, y = Frequency)) +
  geom_bar(stat = "identity", fill = "purple") +
  labs(title = "Distribution of View Quality",
       x = "View Quality",
       y = "Frequency",
       caption = generate_figure_caption("Distribution of View Quality"))
```

The bar plot above displays the distribution of the `view` variable, showing the frequency of different view quality ratings.

#### Price vs. Condition

```{r price_vs_condition, echo = FALSE, include = TRUE}
# Convert condition categories from dummy variables to a factor
train_df_non_linear$condition_category <- factor(apply(train_df_non_linear[, c("condition_1", "condition_2", "condition_3", "condition_4", "condition_5")], 1, function(x) which(x == 1)),
                                                 labels = c("Condition 1", "Condition 2", "Condition 3", "Condition 4", "Condition 5"))

# Create the boxplot with ggplot2
ggplot(train_df_non_linear, aes(x = condition_category, y = price)) +
  geom_boxplot(fill = "blue") +
  labs(title = "Price vs. Condition",
       x = "Condition",
       y = "Price",
       caption = generate_figure_caption("Boxplot of Price vs. Condition"))
```

In the scatter plot above, we compare `price` against the `condition` variable, which represents the condition of the property. This analysis helps us explore how property condition relates to home prices.

```{r distribution_condition, echo = FALSE, include = TRUE}
# Calculate frequencies of each condition rating
condition_frequencies <- colSums(train_df_non_linear[, c("condition_1", "condition_2", "condition_3", "condition_4", "condition_5")])

# Convert frequencies to a data frame for ggplot2
condition_df <- data.frame(Condition = names(condition_frequencies), Frequency = condition_frequencies)

# Create the bar plot with ggplot2
ggplot(condition_df, aes(x = Condition, y = Frequency)) +
  geom_bar(stat = "identity", fill = "green") +
  labs(title = "Distribution of Condition",
       x = "Condition Rating",
       y = "Frequency",
       caption = generate_figure_caption("Distribution of Condition"))
```

The bar plot above visualizes the distribution of the `condition` variable, showing the frequency of different condition ratings.

#### Price vs. Grade

```{r price_vs_grade, echo = FALSE, include = TRUE}
# First, identify all grade-related columns in the dataframe
grade_columns <- grep("grade_", names(train_df_non_linear), value = TRUE)

# Convert dummy variables back to a single categorical variable representing the grade
train_df_non_linear$grade_category <- apply(train_df_non_linear[, grade_columns], 1, function(row) {
  if (all(is.na(row))) {
    return(NA)  # Return NA if all values in the row are NA
  } else {
    idx <- which(row == 1, arr.ind = TRUE)
    return(if(length(idx) > 0) idx else NA)  # Return the index of the grade, or NA if none is 1
  }
})

# Extract grade labels from column names, replacing underscores with hyphens for better readability
grade_labels <- sub("grade_", "", grade_columns) # Remove 'grade_' prefix
grade_labels <- gsub("_", "-", grade_labels) # Replace underscores with hyphens

# Create a boxplot of Price vs. Grade
ggplot(train_df_non_linear, aes(x = factor(grade_category, labels = grade_labels), y = price)) +
  geom_boxplot(fill = "green") +
  labs(title = "Price vs. Grade",
       x = "Grade",
       y = "Price",
       caption = generate_figure_caption("Boxplot of Price vs. Grade"))
```

The scatter plot above compares `price` against the `grade` variable, which has been aggregated into categories as per the provided header. This analysis helps us explore how the grade of construction and design impacts home prices.

```{r distribution_grade, echo = FALSE, include = TRUE}
# Histogram for the Distribution of Grade
# Convert the grade category to a numeric variable for histogram plotting
train_df_non_linear$grade_category_numeric <- as.numeric(train_df_non_linear$grade_category)

# Define breaks for histogram
num_breaks <- length(unique(train_df_non_linear$grade_category_numeric, na.rm = TRUE))
hist_breaks <- seq(min(train_df_non_linear$grade_category_numeric, na.rm = TRUE) - 0.5,
                   max(train_df_non_linear$grade_category_numeric, na.rm = TRUE) + 0.5,
                   length.out = num_breaks + 1)

# Create a histogram with ggplot2
ggplot(train_df_non_linear, aes(x = grade_category_numeric)) +
  geom_histogram(fill = "purple", breaks = hist_breaks) +
  scale_x_continuous(breaks = seq_along(grade_labels), labels = grade_labels) +
  labs(title = "Distribution of Grade",
       x = "Grade",
       y = "Frequency",
       caption = generate_figure_caption("Histogram of Distribution of Grade"))
```

The bar plot above displays the distribution of the `grade_category` variable, showing the frequency of different grade categories.

### Correlation Analysis

Understanding how continuous variables correlate with each other and, more importantly, with the target variable `price`.

```{r correlation_analysis, echo = FALSE, include = TRUE}
# Correlation Matrix of Numeric Variables
cor_matrix <- cor(train_df_non_linear[sapply(train_df_non_linear, is.numeric)])
```

```{r correlation_matrix_table, echo = FALSE, include = TRUE, results = 'asis'}
# Create a table of sorted correlation values
cor_table <- as.data.frame(sort(cor_matrix[,"price"], decreasing = TRUE))

# Display the top 20 correlation values
top_20_corr <- cor_table[1:20, , drop = FALSE]

# Specify the height for the scrollable div
table_height <- "400px"

# Create a scrollable div and place the table inside it
cat(sprintf('<div style="overflow-y: scroll; max-height: %s;">', table_height))
kable(top_20_corr, col.names = c("Variable", "Correlation with Price"), caption = generate_figure_caption("Top 20 Correlation Values with Price"))
cat('</div>')
```

```{r correlation_heatmap, echo = FALSE, include = TRUE}
# Heatmap of the top 20 correlation values
# Filter the top 20 correlation values
top_20_corr_variables <- rownames(top_20_corr)
top_20_corr_matrix <- cor_matrix[top_20_corr_variables, top_20_corr_variables]

# Create a heatmap
ggplot(melt(top_20_corr_matrix), aes(Var1, Var2, fill = value)) +
  geom_tile() +
  labs(title = "Top 20 Correlations",
       x = "Variable",
       y = "Variable",
       caption = generate_figure_caption("Heatmap showing the top 20 correlations")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r correlation_matrix_for_multicollinearity, echo = FALSE, include = FALSE}
# Selecting predictors and excluding the response variable 'price'
predictors <- dplyr::select(train_df_linear, -price)

# Convert factors to numeric
numeric_predictors <- predictors %>%
  mutate(across(where(is.factor), as.numeric)) %>%
  mutate(across(where(is.character), ~as.numeric(as.factor(.))))

# Calculate the correlation matrix
corr_matrix <- cor(numeric_predictors, use = "pairwise.complete.obs")

# Convert the correlation matrix to a long format
correlated_pairs_df <- melt(corr_matrix)

# Filter out redundant pairs (keep only lower triangle of the matrix)
correlated_pairs_df <- correlated_pairs_df %>%
  filter(Var1 != Var2) %>%  # Remove self-correlations
  filter(abs(value) > 0.8) %>%
  filter(match(Var1, rownames(corr_matrix)) < match(Var2, rownames(corr_matrix)))

# Rename columns for clarity
correlated_pairs_df <- correlated_pairs_df %>%
  rename(Variable1 = Var1, Variable2 = Var2, Correlation = value)

# Print the correlated pairs as a kable table with a caption
knitr::kable(correlated_pairs_df, caption = generate_figure_caption("Correlated Pairs with Absolute Correlation > 0.8"))
```

```{r corr_removal_table, echo = FALSE, include = TRUE, results = 'asis'}
# Output the table of highly correlated pairs using knitr::kable()
knitr_table <- kable(
  correlated_pairs_df,
  caption = generate_figure_caption("Highly Correlated Variable Pairs"),
  format = "markdown"
)
print(knitr_table)
```
#### Detailed Explanation for Removal

1. **`sqft_above` & `sqft_living`**: Both variables are highly correlated because the square footage of the living area above ground (`sqft_above`) is part of the total square footage of living space (`sqft_living`). We remove `sqft_above` as it is likely to contain less unique information than the total living space.

2. **`week_of_year`, `day_of_year`, `month_sold`**: These variables are related to the date the house was sold and are thus inherently correlated. `day_of_year` carries the most granular information, so we might prefer to keep it and remove `week_of_year` and `month_sold` which provide more aggregated temporal information.

3. **`condition_4` & `condition_3`**: The condition of the house is a categorical variable that has been one-hot encoded. Since these are mutually exclusive categories, they are negatively correlated. We might decide to keep one category as the reference group and remove the others, or revert to the original categorical variable to capture the overall condition in a single variable.

By removing these variables, we aim to reduce multicollinearity, which can distort the estimated regression coefficients, inflate standard errors, and undermine the statistical significance of the predictors. The goal is to retain the variables that provide the unique and informative contribution to the model's prediction of house prices.

#### Correlation Graphics Analysis

In the table above, we've displayed the top 20 correlation values with the target variable `price`, sorted by their absolute values. Here are some of the key findings:

   1. **Positive Correlations with Price**:
      - Variables such as `sqft_living`, `sqft_above`, `sqft_living15`, and `bathrooms` exhibit strong positive correlations with the target variable. This suggests that as these variables increase, the house price tends to increase as well.
      - Features like `grade_11_13`, `view_4`, and `grade_8_10` also show positive correlations, indicating that higher-grade properties and better views tend to have higher prices.

   2. **Negative Correlations with Price**:
      - No negative correlations are present in the top 20. This means that none of the examined features strongly suggest a decrease in price as they increase.

   3. **Feature Importance**:
      - The strength of the correlations helps us understand the importance of these variables in predicting house prices. Features like `sqft_living` and `grade_11_13` appear to be strong predictors of price.
      - Variables related to location, such as `zipcode_98004`, `zipcode_98039`, and `zipcode_98040`, also have notable positive correlations, indicating the significance of location in price determination.

### Temporal Trends Analysis

Analyzing the influence of time-related features such as `month`, and `season` on house prices.

#### Monthly Trends in Average House Prices

```{r monthly_trends_in_average_house_prices, echo = FALSE, include = TRUE}
# Monthly Trends in Average House Prices
monthly_trends <- aggregate(train_df_non_linear$price, by = list(train_df_non_linear$month_sold), FUN = mean)
colnames(monthly_trends) <- c("Month", "Average_Price")

# Find the global maximum
global_max <- monthly_trends[which.max(monthly_trends$Average_Price), ]

ggplot(monthly_trends, aes(x = Month, y = Average_Price)) +
  geom_line() +
  geom_text(data = global_max, aes(x = Month, y = Average_Price, label = paste("Global Max:", round(Average_Price, 2))), vjust = -0.5) +
  labs(title = "Monthly Trends in Average House Prices", x = "Month", y = "Average Price", caption = generate_figure_caption("Monthly Trends in Average House Prices")) +
  scale_x_continuous(breaks = 1:12, labels = month.name) +
  theme_minimal()
```

```{r monthly_trends_count, echo = FALSE, include = TRUE}
# Monthly Trends in Count of Homes Sold
monthly_counts <- table(train_df_non_linear$month_sold)
months <- factor(1:12, labels = month.name)
monthly_counts_df <- data.frame(Month = months, Count = as.numeric(monthly_counts))

# Find the global maximum
global_max_count <- monthly_counts_df[which.max(monthly_counts_df$Count), ]

ggplot(monthly_counts_df, aes(x = Month, y = Count, group = 1)) +
  geom_bar(stat = "identity") +
  geom_text(data = global_max_count, aes(x = Month, y = Count, label = paste("Global Max:", Count)), vjust = -0.5) +  # Add label for global maximum
  labs(title = "Monthly Trends in Count of Homes Sold", x = "Month", y = "Count of Homes Sold", caption = generate_figure_caption("Monthly Trends in Count of Homes Sold")) +
  theme_minimal()
```

#### Seasonal Trends in Average House Prices

```{r seasonal_trends_in_average_house_prices, echo = FALSE, include = TRUE}
# Aggregate average price for each season
seasonal_trends <- data.frame(
  Season = c("Winter", "Spring", "Summer", "Fall"),
  Average_Price = c(
    mean(train_df_non_linear$price[train_df_non_linear$season_Winter == 1]),
    mean(train_df_non_linear$price[train_df_non_linear$season_Spring == 1]),
    mean(train_df_non_linear$price[train_df_non_linear$season_Summer == 1]),
    mean(train_df_non_linear$price[train_df_non_linear$season_Fall == 1])
  )
)

# Find the global maximum
global_max_seasonal <- seasonal_trends[which.max(seasonal_trends$Average_Price), ]

# Plotting
ggplot(seasonal_trends, aes(x = Season, y = Average_Price, fill = Season)) +
  geom_bar(stat = "identity") +
  geom_text(data = global_max_seasonal, aes(label = paste("Global Max:", round(Average_Price, 2)), y = Average_Price), vjust = -0.5) +
  labs(title = "Seasonal Trends in Average House Prices", x = "Season", y = "Average Price", caption = generate_figure_caption("Seasonal Trends in Average House Prices")) +
  theme_minimal()

```

```{r seasonal_trends_count, echo = FALSE, include = TRUE}
# Count homes sold for each season
seasonal_counts <- c(
  sum(train_df_non_linear$season_Winter == 1),
  sum(train_df_non_linear$season_Spring == 1),
  sum(train_df_non_linear$season_Summer == 1),
  sum(train_df_non_linear$season_Fall == 1)
)
seasonal_counts_df <- data.frame(Season = c("Winter", "Spring", "Summer", "Fall"), Count = seasonal_counts)

# Find the global maximum
global_max_count_seasonal <- seasonal_counts_df[which.max(seasonal_counts_df$Count), ]

# Plotting
ggplot(seasonal_counts_df, aes(x = Season, y = Count, fill = Season)) +
  geom_bar(stat = "identity") +
  geom_text(data = global_max_count_seasonal, aes(label = paste("Global Max:", Count), y = Count), vjust = -0.5) +
  labs(title = "Seasonal Trends in Count of Homes Sold", x = "Season", y = "Count of Homes Sold", caption = generate_figure_caption("Seasonal Trends in Count of Homes Sold")) +
  theme_minimal()

```

#### Week of the Year Trends in Average House Prices

```{r week_of_the_year_trends_in_average_house_prices, echo = FALSE, include = TRUE}
# Week of the Year Trends in Average House Prices
weekly_trends <- aggregate(train_df_non_linear$price, by = list(train_df_non_linear$week_of_year), FUN = mean)
colnames(weekly_trends) <- c("Week_of_Year", "Average_Price")

# Find the global maximum
global_max_weekly <- weekly_trends[which.max(weekly_trends$Average_Price), ]

ggplot(weekly_trends, aes(x = Week_of_Year, y = Average_Price)) +
  geom_line() +
  geom_text(data = global_max_weekly, aes(x = Week_of_Year, y = Average_Price, label = paste("Global Max:", round(Average_Price, 2))), vjust = -0.5) +  # Add label for global maximum
  labs(title = "Weekly Trends in Average House Prices", x = "Week of Year", y = "Average Price", caption = generate_figure_caption("Weekly Trends in Average House Prices")) +
  theme_minimal()
```

```{r week_of_the_year_trends_count, echo = FALSE, include = TRUE}
# Weekly Trends in Count of Homes Sold
weekly_counts <- table(train_df_non_linear$week_of_year)
weekly_counts_df <- data.frame(Week_of_Year = as.numeric(names(weekly_counts)), Count = as.numeric(weekly_counts))

# Find the global maximum
global_max_count_weekly <- weekly_counts_df[which.max(weekly_counts_df$Count), ]

ggplot(weekly_counts_df, aes(x = Week_of_Year, y = Count)) +
  geom_bar(stat = "identity") +
  geom_text(data = global_max_count_weekly, aes(x = Week_of_Year, y = Count, label = paste("Global Max:", Count)), vjust = -0.5) +  # Add label for global maximum
  labs(title = "Weekly Trends in Count of Homes Sold", x = "Week of Year", y = "Count of Homes Sold", caption = generate_figure_caption("Weekly Trends in Count of Homes Sold")) +
  theme_minimal()
```

#### Day of the Year Trends in Average House Prices

```{r day_of_the_year_trends_in_average_house_prices, echo = FALSE, include = TRUE}
# Day of the Year Trends in Average House Prices
daily_trends <- aggregate(train_df_non_linear$price, by = list(train_df_non_linear$day_of_year), FUN = mean)
colnames(daily_trends) <- c("Day_of_Year", "Average_Price")

# Find the global maximum
global_max_daily <- daily_trends[which.max(daily_trends$Average_Price), ]

ggplot(daily_trends, aes(x = Day_of_Year, y = Average_Price)) +
  geom_line() +
  geom_text(data = global_max_daily

, aes(x = Day_of_Year, y = Average_Price, label = paste("Global Max:", round(Average_Price, 2))), vjust = -0.5) +  # Add label for global maximum
  labs(title = "Daily Trends in Average House Prices", x = "Day of Year", y = "Average Price", caption = generate_figure_caption("Daily Trends in Average House Prices")) +
  theme_minimal()
```

```{r day_of_the_year_trends_count, echo = FALSE, include = TRUE}
# Daily Trends in Count of Homes Sold
daily_counts <- table(train_df_non_linear$day_of_year)
daily_counts_df <- data.frame(Day_of_Year = as.numeric(names(daily_counts)), Count = as.numeric(daily_counts))

# Find the global maximum
global_max_count_daily <- daily_counts_df[which.max(daily_counts_df$Count), ]

ggplot(daily_counts_df, aes(x = Day_of_Year, y = Count)) +
  geom_bar(stat = "identity") +
  geom_text(data = global_max_count_daily, aes(x = Day_of_Year, y = Count, label = paste("Global Max:", Count)), vjust = -0.5) +  # Add label for global maximum
  labs(title = "Daily Trends in Count of Homes Sold", x = "Day of Year", y = "Count of Homes Sold", caption = generate_figure_caption("Daily Trends in Count of Homes Sold")) +
  theme_minimal()
```

- The "Daily Trends in Average House Prices" line chart showcases the average house prices for each day of the year. It helps identify daily patterns and potential price variations that could be influenced by specific dates or events.

### Geographical Influence Analysis

Investigating the spatial aspect by analyzing the `distance_to_convergence` variable.

```{r geographical_price_analysis, echo = FALSE, include = TRUE, results = 'asis'}
# Calculate z-scores for the prices
train_df_non_linear <- train_df_non_linear %>%
  mutate(z_score = scale(price))

# Define z-score intervals and corresponding colors
z_score_intervals <- seq(-3, 3, by = 1)  # Create a sequence of z-scores from -3 to 3
color_sequence <- c("green", "#8fd744", "#fde725", "#f76818ff", "#d7301fff", "#440154")  # From green to dark color

# Calculate price at each z-score interval
price_at_intervals <- sapply(z_score_intervals, function(z) {
  mean(train_df_non_linear$price) + z * sd(train_df_non_linear$price)
})

# Ensure breaks are in ascending order and rounded to the nearest 25k
breaks <- sort(round(price_at_intervals / 25000) * 25000)
breaks <- c(min(train_df_non_linear$price, na.rm = TRUE), breaks, max(train_df_non_linear$price, na.rm = TRUE))

# If there are negative values or values that don't make sense, remove them
breaks <- breaks[breaks >= 0]

# Create color palette with a color for each interval
color_palette <- colorBin(color_sequence, domain = train_df_non_linear$price, bins = breaks, na.color = "#808080")

# Initialize the leaflet map with updated color palette
m <- leaflet(train_df_non_linear) %>%
  addTiles() %>%
  addCircleMarkers(
    lat = ~lat, lng = ~long,
    color = ~color_palette(price),
    fillColor = ~color_palette(price),
    fillOpacity = 0.8,
    radius = 1,  # Small dots
    popup = ~paste("Price: $", formatC(price, format = "f", big.mark = ","), "<br>", "Z-Score: ", round(z_score, 2))
  )

# Define the maximum distance for the distance bands
max_distance <- max(train_df_non_linear$distance_to_convergence, na.rm = TRUE)

# Add distance bands to the map
for (i in seq(2, max_distance, by = 2)) {
  m <- addCircles(m, lat = convergence_point[1], lng = convergence_point[2], radius = i * 1000,
                  color = "grey", weight = 1, fill = FALSE, dashArray = "5, 5")
}

# Add legend and finalize the map
m <- m %>%
  addLegend(
    position = "bottomright",
    pal = color_palette,
    values = ~price,
    title = "Price",
    labFormat = labelFormat(prefix = "$"),
    opacity = 1
  ) %>%
  setView(lng = convergence_point[2], lat = convergence_point[1], zoom = 10)

cat(generate_figure_caption('Distance to Convergence Map'))
# Print the map
m
```

### Conclusion
This detailed review of the King County house sales dataset underscores the thorough preparation undertaken for the predictive analysis. The dataset's diverse variables, both continuous and categorical, have been meticulously processed and analyzed, providing a robust foundation for developing the predictive model. With the comprehensive EDA and graphical analysis, we gain valuable insights into the correlations and distributions within the data, setting the stage for effective model building and accurate house price prediction.

## III. Model Development Process

Up to this point, we have successfully conducted an exploratory data analysis (EDA) to gain valuable insights into the dataset. We've visualized key features such as price, bedrooms, bathrooms, and more, allowing us to better understand the data's distribution and relationships. Additionally, we've explored various trends, including monthly, seasonal, weekly, and daily trends in both house prices and the count of homes sold. Furthermore, we have cleaned and prepared the data, removing irrelevant variables like `id`, `lat`, and `long` to streamline it for modeling. With these preliminary steps completed, we are now ready to delve into the model development process.

### Initial OLS Model
To commence the model development process, we establish an Ordinary Least Squares (OLS) regression model as our baseline. This initial model utilizes the features that have undergone transformation and cleaning during the exploratory data analysis (EDA) phase. To maintain data quality, enhance model performance, and facilitate interpretability, we begin by removing columns introduced in prior graphical iterations. Additionally, we employ the standard data preprocessing practice of dropping columns with missing values (NA) to ensure dataset integrity. This step ensures that our subsequent analyses and models are built upon a robust and complete dataset, minimizing errors and potential biases.   bn

```{r initial_model, echo = FALSE, include = FALSE}
# Drop columns created for visualizations in prior steps, and columns that have almost perfect correlation and multicollinearity.
train_df_non_linear <- train_df_non_linear[, !colnames(train_df_non_linear) %in% c("view_category", "condition_category", "grade_category", "grade_category_numeric", "z_score", "lat", "long", 'sqft_above', 'month_sold', 'week_of_year', 'condition_3', "grade_Below_Average", "bedrooms_factor", "bathrooms_factor")]
train_df_linear <- train_df_linear[, !colnames(train_df_linear) %in% c("view_category", "condition_category", "grade_category", "grade_category_numeric", "z_score", "lat", "long", 'sqft_above', 'month_sold', 'week_of_year', 'condition_3', "grade_Below_Average", "bedrooms_factor", "bathrooms_factor")]
test_df_linear <- test_df_linear[, !colnames(test_df_linear) %in% c("view_category", "condition_category", "grade_category", "grade_category_numeric", "z_score", "lat", "long", 'sqft_above', 'month_sold', 'week_of_year', 'condition_3', "grade_Below_Average", "bedrooms_factor", "bathrooms_factor")]
test_df_non_linear <- test_df_non_linear[, !colnames(test_df_non_linear) %in% c("view_category", "condition_category", "grade_category", "grade_category_numeric", "z_score", "lat", "long", 'sqft_above', 'month_sold', 'week_of_year', 'condition_3', "grade_Below_Average", "bedrooms_factor", "bathrooms_factor")]

#' Add Model Performance to Dataframe
#'
#' This function evaluates a given regression model using training and testing datasets. It calculates
#' performance metrics like SSE, R-squared, RMSE, and MAE for both training and test sets, and then adds
#' these metrics to a provided results dataframe or creates one if not provided. The function ensures
#' that both training and testing datasets contain the same features used in the model before performing
#' predictions and calculations.
#'
#' @param model_name A string representing the name of the model.
#' @param model The model object to be evaluated.
#' @param x_train A dataframe containing the features of the training data.
#' @param y_train A vector containing the target variable of the training data.
#' @param x_test A dataframe containing the features of the test data.
#' @param y_test A vector containing the target variable of the test data.
#' @param df_results An optional dataframe where model performance metrics will be added (default: NULL).
#' @return The updated dataframe with the added model performance metrics.
#'
#' @examples
#' linear_model <- lm(price ~ ., data = train_df)
#' df_results <- add_model_performance("Linear Model", linear_model, x_train, y_train, x_test, y_test)
#'
#' # If df_results already exists and you want to add more results:
#' df_results <- add_model_performance("Another Model", another_model, x_train, y_train, x_test, y_test, df_results)
#'
#' @details
#' The function first extracts the features used in the model and checks if these features are present in
#' both the training and testing datasets. It then uses the model to predict the target variable on both
#' datasets and calculates performance metrics. These metrics are added to a dataframe that either exists
#' or is created within the function. This dataframe can be used for comparing different models' performances.
#'
#' It is assumed that the model is correctly specified with the appropriate features and the dataframes
#' provided to the function align with the model's structure.

add_model_performance <- function(model_name, model, x_train, y_train, x_test, y_test, df_results = NULL) {
    # Create df_results if not provided
    if (is.null(df_results)) {
        df_results <- data.frame(
            Model = character(),
            SSE_train = double(),
            SSE_test = double(),
            R_squared_train = double(),
            R_squared_test = double(),
            RMSE_train = double(),
            RMSE_test = double(),
            MAE_train = double(),
            MAE_test = double(),
            stringsAsFactors = FALSE
        )
    }

    # Perform predictions using the model
    y_hat_train <- predict(model, newdata = x_train)
    y_hat_test <- predict(model, newdata = x_test)

    # Performance metrics calculation
    mae_train <- mean(abs(y_train - y_hat_train))
    mae_test <- mean(abs(y_test - y_hat_test))

    sse_train <- sum((y_train - y_hat_train)^2)
    sse_test <- sum((y_test - y_hat_test)^2)

    tss_train <- sum((y_train - mean(y_train))^2)
    tss_test <- sum((y_test - mean(y_test))^2)

    rsq_train <- 1 - (sse_train / tss_train)
    rsq_test <- 1 - (sse_test / tss_test)

    rmse_train <- sqrt(mean((y_train - y_hat_train)^2))
    rmse_test <- sqrt(mean((y_test - y_hat_test)^2))

    # Appending results to the dataframe
    new_row <- data.frame(
        Model = model_name,
        SSE_train = sse_train,
        SSE_test = sse_test,
        R_squared_train = rsq_train,
        R_squared_test = rsq_test,
        RMSE_train = rmse_train,
        RMSE_test = rmse_test,
        MAE_train = mae_train,
        MAE_test = mae_test
    )

    df_results <- rbind(df_results, new_row)

    # Returning the updated dataframe
    return(df_results)
}


#' Display Model Results using Datatable
#'
#' This function displays a dataframe containing model performance metrics using the `datatable` function.
#'
#' @param df_results A dataframe containing model performance metrics.
#' @param caption Optional caption for the table (default: "Model Comparison").
#' @return NULL (it displays the table but doesn't return a value).

view_model_results <- function(df_results, caption = "Model Comparison") {
  # Identifying numeric columns other than "Model", "R_squared_train", and "R_squared_test"
  cols_to_round <- setdiff(names(df_results[sapply(df_results, is.numeric)]), c("Model", "R_squared_train", "R_squared_test"))

  # Round these specific columns to 2 decimal places
  df_results[cols_to_round] <- lapply(df_results[cols_to_round], round, 2)

  # Round R-squared columns to 4 decimal places
  df_results$R_squared_train <- round(df_results$R_squared_train, 5)
  df_results$R_squared_test <- round(df_results$R_squared_test, 5)

  # Generate the caption using generate_figure_caption function
  full_caption <- generate_figure_caption(caption)

  # Display the dataframe using datatable
  datatable(
    df_results,
    caption = full_caption,
    options = list(
      paging = FALSE,
      autoWidth = TRUE,
      scrollX = TRUE,
      fixedColumns = list(leftColumns = 1)
    )
  )
}


extract_and_display_features <- function(model, full_df, target_var) {
  # Extract features used in the model (excluding the intercept)
  features_used <- names(coef(model))
  features_used <- features_used[features_used != "(Intercept)"]

  # Get all the feature names from the full dataset, excluding the target variable
  all_features <- names(full_df)
  all_features <- all_features[all_features != target_var]

  # Identify features not used in the model
  unused_features <- setdiff(all_features, features_used)

  # # USE THIS FOR DEBUGGING ISSUES
  # # Display the unused features
  # if (length(unused_features) > 0) {
  #   cat("Features not used in the model:\n", paste(unused_features, collapse = ", "), "\n")
  # } else {
  #   cat("No features were dropped in the model.\n")
  # }

  # Return the features that were used as a character vector
  return(features_used)
}

# Function to create a linear model based on a subset of features
create_model <- function(df, target_var, features) {
  # Construct the formula for lm()
  formula <- as.formula(paste(target_var, "~", paste(features, collapse = "+")))
  # Fit the linear model
  return(lm(formula, data = df))
}

prepare_data <- function(model, train_df, test_df, target_var) {
  # Extracting used features from the model
  used_features <- extract_and_display_features(model, train_df, target_var = target_var)

  # Create x_train and x_test without the target variable
  x_train <- subset(train_df, select = used_features)
  x_test <- subset(test_df, select = used_features)

  y_train <- train_df[[target_var]]
  y_test <- test_df[[target_var]]

  return(list(x_train = x_train, x_test = x_test, y_train = y_train, y_test = y_test))
}

evaluate_model <- function(model_name, model, train_df, test_df, target_var, df_results) {
  # Extracting used features from the model
  used_features <- extract_and_display_features(model, train_df, target_var = target_var)

  # Filter data into x and y subsets
  data <- prepare_data(model, train_df, test_df, target_var = target_var)

  # Adding model performance to the results dataframe
  df_results <- add_model_performance(
    model_name = model_name,
    model = model,
    x_train = data$x_train,
    y_train = data$y_train,
    x_test = data$x_test,
    y_test = data$y_test,
    df_results = df_results  # Or pass existing df_results if available
  )

  return(df_results)
}
```

```{r build_inital_linear_model}
# Fit a linear regression model to the training data
linear_model_initial <- lm(price ~ ., data = train_df_linear)

# Evaluate OLS_linear
df_results <- evaluate_model("OLS_linear", linear_model_initial, train_df_linear, test_df_linear, target_var = 'price', NULL)

# Show inital linear model results
summary(linear_model_initial)

view_model_results(df_results, caption = "OLS Linear Model Table")
```


### Detailed Initial Model Insights

#### Model Fit
- **R-squared (0.8404)**: Approximately 84.04% of the variability in real estate prices is explained by the model, indicating a strong fit.
- **Adjusted R-squared (0.8393)**: Adjusted for the number of predictors, this value reaffirms the model’s effectiveness.

#### Coefficient Analysis
- **Significant Predictors**: Features like `sqft_living`, `waterfront1`, and `bedrooms` have significant coefficients, implying a notable impact on housing prices.
- **Insignificant Predictors**: Variables such as `sqft_lot15` show less significance, suggesting a minor influence on price.

#### Grade Variable Insights
- **Grade Variables**: The negative coefficients for grades (`grade_3` to `grade_12`) compared to a baseline grade (the omitted variable) are intriguing. This suggests that higher grades (implying better quality) are associated with lower prices, which warrants further investigation for data inconsistencies or other underlying factors.

#### Outliers and Residuals
- **Large Residuals**: The considerable spread in residuals (from -1518789 to 3645881) may indicate the presence of outliers or non-linear relationships not captured by the model.

### Model Diagnostics and Assumptions

#### Checking Model Assumptions

As we scrutinize our OLS model's assumptions through diagnostic plotting, we encounter significant insights that warrant our attention and action.

##### Residuals vs Fitted Plot

The residuals vs fitted plot is our first stop to assess the assumption of linearity. The ideal scenario is a random spread of residuals around the horizontal axis, indicating a linear relationship between predictors and the response.

```{r residuals_vs_fitted_plot, echo = FALSE, include = TRUE, results = 'asis'}
caption <- generate_figure_caption("Residuals vs Fitted Plot for Linear Model")
cat(caption)
plot(linear_model_initial, 1)
```

**Observation**: A discernible pattern in the residuals suggests that our model may not be capturing some of the non-linear relationships.
**Points of Concern**: Specific points, notably labeled 431, 9381, and 11944, stray from the general cluster, hinting at potential outliers or influential observations.

##### Normal Q-Q Plot

The Q-Q plot offers a visual comparison of the distribution of residuals against a perfectly normal distribution. Deviations from the diagonal indicate departures from normality.

```{r QQ_plot, echo = FALSE, include = TRUE, results = 'asis'}
caption <- generate_figure_caption("QQ Plot for Linear Model")
cat(caption)
plot(linear_model_initial, 2)
```

**Observation**: The tails of our plot, particularly the right tail, show a clear departure from normality, with several points veering off the expected line.
**Specific Note**: The points at the upper end, again including 431, 9381, and 11944, are especially distant from the line, suggesting they could be outliers with a substantial effect on our model's performance.

##### Scale-Location Plot

Also known as the spread-location plot, it's used to check for equal variance of residuals (homoscedasticity).

```{r scale_location_plot, echo = FALSE, include = TRUE, results = 'asis'}
caption <- generate_figure_caption("Scale-Location Plot for Linear Model")
cat(caption)
plot(linear_model_initial, 3)

```

**Observation**: The red line's upward trajectory indicates an increase in the spread of residuals as the fitted values rise, which is a classic sign of heteroscedasticity.

##### Residuals vs Leverage Plot

This plot helps us identify influential data points that could unduly sway our model's predictions.

```{r residuals_vs_leverage_plot, echo = FALSE, include = TRUE, results = 'asis'}
caption <- generate_figure_caption("Residuals vs Leverage Plot for Linear Model")
cat(caption)
plot(linear_model_initial, 5)
```

**Observation**: Several points fall outside the Cook's distance lines, with points 3405, 9381 and 11944 appearing as particularly influential to our regression estimates.

##### Cook's Distance Plot

The Cook's distance plot is instrumental in quantifying the influence of each data point.

```{r cooks_distance_plot, echo = FALSE, include = TRUE, results = 'asis'}
caption <- generate_figure_caption("Cooks Distance Plot for Linear Model")
cat(caption)
plot(linear_model_initial, 4)
```

**Observation**: The spikes in the plot, notably for points 3405, 9381, and 11944, surpass the typical threshold, suggesting they are disproportionately influencing the model.

### Next Steps in Model Diagnostics

Having thoroughly examined the diagnostic plots for our OLS regression model, we have identified several areas requiring intervention to ensure the robustness of our model. Here's how we propose to address these:

#### Addressing Non-linearity

The Residuals vs Fitted plot signaled potential non-linearity, which could be corrected by:

- **Integrating Polynomial Terms**: We should experiment with adding polynomial terms for continuous predictors to capture any non-linear relationships.
- **Exploring Interaction Effects**: If certain predictors interact in a non-additive manner, including interaction terms might offer a better fit.

#### Stabilizing Variance

The Scale-Location plot indicated the presence of heteroscedasticity, suggesting our model's residuals have non-constant variance. To mitigate this:

- **Transformation of Response Variable**: We could apply a log transformation to the response variable, `price`, which often stabilizes variance in the presence of heteroscedasticity.
- **Implementing Weighted Regression**: A weighted least squares approach could also be employed, giving different weights to observations based on their fitted values.

#### Normalizing Residuals

The Normal Q-Q plot showed deviations from normality, especially in the tails, prompting us to consider:

- **Applying Transformations**: Besides log transformations, other transformations like square root or inverse could be applied to achieve normality in the residuals.
- **Leveraging Non-linear Models**: Should transformations fail to yield normally distributed residuals, we might pivot to non-linear models better suited to the data structure.

#### Managing Influential Observations

The Residuals vs Leverage plot, coupled with the Cook's Distance plot, highlighted observations with undue influence on our model's estimates. We'll approach this by:

- **Investigating High-leverage Points**: Points like 3405, 9381, and 11944 require close examination to determine their legitimacy and impact on the model.
- **Considering Removal or Retention**: Post investigation, we'll make informed decisions about removing these points or alternatively, leveraging robust regression techniques to diminish their influence.

#### Refinement and Validation

As we implement these adjustments, we will:

- **Continually Refine Our Model**: Each change brings us closer to a model that accurately captures the underlying data structure.
- **Rigorously Validate Adjustments**: It's vital to validate our model using cross-validation techniques to ensure our changes generalize well to unseen data.

Through these methodical steps, we aim to enhance the reliability and validity of our model's predictive power, ensuring that it not only meets statistical assumptions but also aligns with empirical data.

### Outlier Detection

To further investigate nonlinearity, constant variance of errors, and outliers, we examine the data points with potentially high influence.

#### Residuals vs Leverage Plot

We use the `ols_plot_resid_lev` function to create a plot for detecting outliers and observations with high leverage.

```{r residuals_vs_leverage_plot_v2, echo=FALSE, include = TRUE, results = 'asis'}

# Output figure and caption
cat(generate_figure_caption("OLS Outlier and Leverage Diagnostics Linear Model"))
# Plot the residuals vs leverage plot in the first row
ols_plot_resid_lev(linear_model_initial)
```

- **Observation**: Look for observations that have high leverage (extreme values on the right side of the plot) and potentially strong influence on the model.

#### Residuals vs Fitted Plot

We use the `ols_plot_resid_stud_fit` function to create a plot that helps detect non-linearity, constant variances, and outliers in residuals.

```{r residuals_vs_fitted_plot_v2, echo=FALSE, include = TRUE, results = 'asis'}
# Output figure and caption
cat(generate_figure_caption("Deleted Studentized Resid. vs Pred. Linear Model"))

ols_plot_resid_stud_fit(linear_model_initial)
```

- **Observation**:

#### Cook's Distance

We calculate Cook's distance for each observation to identify influential data points.

```{r cooks_distance_calculation, echo=FALSE, include = TRUE, results = 'asis'}
# Calculate Cook's distance for each observation
cooksd <- cooks.distance(linear_model_initial)

# Output figure and caption
cat(generate_figure_caption("OLS Cooks Distance Plot for Linear Model"))

# Create a plot of Cook's distance
plot(cooksd, type = "h", main = "Cook's Distance", ylab = "Cook's Distance", xlab = "Observation Index")
abline(h = 0.04, col = "red", lwd = 2) # .04 is a common threshold for Cook's distance

# Define the threshold for Cook's distance
threshold <- 0.04

# Identify observations where Cook's distance exceeds the threshold
high_cooksd <- which(cooksd > threshold)

# Add labels to the points above the threshold
text(high_cooksd, cooksd[high_cooksd], labels = high_cooksd, cex = 0.7, pos = 3)
```

- **Observation**:

#### Filtering Outliers

We filter the data points with Cook's distance >= 0.04, which is a reasonable threshold for identifying influential data points.

```{r outlier_filtering, echo=FALSE, include = TRUE}
# Define the threshold for Cook's distance, opting for slightly lower than .04 as it benefits the model performance to do a little bit lower
threshold <- 0.04

# Identify the indices of influential observations
influential_obs <- which(cooksd > threshold)

print(influential_obs)
```


```{r outlier_removal}
# Remove outliers from both datasets
train_df_linear_outliers <- train_df_linear[-influential_obs, ]
train_df_non_linear_outliers <- train_df_non_linear[-influential_obs, ]
```

### Diagnostics for Multicollinearity, Homoscedasticity, Normality of Errors and Constancy of Error Variances

### Examining Multicollinearity

```{r vif}
vif_values <- vif(linear_model_initial)

# Set the VIF threshold (e.g., 10)
vif_threshold <- 10

# Function to remove high VIF columns from a dataset
remove_high_vif_columns <- function(df, threshold) {
  linear_model <- lm(price ~ ., data = df)
  vif_values <- vif(linear_model)
  high_vif_columns <- names(vif_values[vif_values > threshold])
  df <- df[, !names(df) %in% high_vif_columns]
  return(df)
}

# Remove high VIF columns from the copied training and test datasets
train_df_linear_vif <- remove_high_vif_columns(train_df_linear, vif_threshold)
test_df_linear_vif <- remove_high_vif_columns(test_df_linear, vif_threshold)

# Fit a linear regression model to the updated training data
linear_model_initial_vif <- lm(price ~ ., data = train_df_linear_vif)

# Evaluate the updated linear model on the copied test data
df_results <- evaluate_model("OLS_linear_VIF_10", linear_model_initial_vif, train_df_linear_vif, test_df_linear_vif, target_var = 'price', df_results)

# Show updated linear model results
summary(linear_model_initial_vif)
view_model_results(df_results, caption = "VIF Multicollinearity Removed")
```

#### Breusch-Pagan Test

We will perform the Breusch-Pagan test to examine if the assumption of homoscedasticity holds in our model. This test will help us understand if the variances of the residuals are constant across levels of the independent variables.

```{r breusch_pagan_test, echo = FALSE}
# Breusch-Pagan Test
bp_test_results <- bptest(linear_model_initial, studentize = FALSE)

# Output the results of the Breusch-Pagan test
bp_test_results
```

The Breusch-Pagan test results with a p-value significantly lower than the alpha level of 0.05 suggests that we reject the null hypothesis of constant error variances in favor of the alternative hypothesis, indicating the presence of heteroscedasticity in our model residuals.

### Weighted Least Squares

```{r weighted_least_squares, echo = FALSE, include = TRUE, results = 'asis'}
# Extract absolute residuals from linear_modle_inital as VIF removal reduced our performance
abs_residuals <- abs(linear_model_initial$residuals)

# Fit a linear regression model to the absolute residuals
abs_residuals_model <- lm(abs_residuals ~ ., data = train_df_linear_vif[, -1])

# Calculate fitted values from the absolute residuals model
fitted_values <- abs_residuals_model$fitted.values

# Calculate weights (w1) based on the squared fitted values
weights <- 1 / (fitted_values^2)

# Fit a weighted linear regression model to the log-transformed price with weights
weighted_regression_model <- lm(transformed_price ~ ., weights = weights, data = train_df_linear_vif)

# Display the summary of the weighted model
summary(weighted_regression_model)

# Create diagnostic plots
par(mfrow=c(2,2))
cat(generate_figure_caption("Diagnostic Results for WLS"))
plot(weighted_regression_model)
par(mfow=c(1,1))
cat(generate_figure_caption("Boxplot of WLS Residuals"))
boxplot(weighted_regression_model$residuals)
```

```{r wls_model_evaluation}
df_results <- NULL

# Perform Breusch-Pagan test for heteroscedasticity
bptest(weighted_regression_model, studentize = TRUE)

ei_trains <- transformed_model$residuals
abs_ei_trans <- abs(ei_trains)
fit_price_res <- lm(abs_ei_trans ~., data = train_df_lin[, -1])
#summary(fit_price_res)
res_fitted_values <- fit_price_res$fitted.values
w1 <- 1/(res_fitted_values^2)

fit_price_train_trans_w1 <- lm(log(price) ~. ,
                               weights = w1, data = df_train_linear_out_step_vif)
```

To further assess the normality of our model's residuals, we will conduct the Anderson-Darling normality test, especially relevant given our large sample size.

```{r anderson_darling_test, echo = FALSE}
# Anderson-Darling normality test
ad_test_results <- ad.test(linear_model_initial$residuals)

# Output the results of the Anderson-Darling test
print(ad_test_results)
```

If the p-value from the Anderson-Darling test is less than our alpha level of 0.05, we will reject the null hypothesis that the errors are normally distributed.

#### Anderson-Darling Test Results

The results from the Anderson-Darling test are as follows:

- **A-squared**: `r ad_test_results$statistic`
- **P-value**: `r ad_test_results$p.value`

Since the p-value is `r ifelse(ad_test_results$p.value < 0.05, "less", "greater")` than 0.05, we `r ifelse(ad_test_results$p.value < 0.05, "reject", "fail to reject")` the null hypothesis, suggesting that `r ifelse(ad_test_results$p.value < 0.05, "errors are not normally distributed", "errors are normally distributed")`.

#### Examining the Non-Linearity of the Model

```{r boxcox, echo = FALSE, include = TRUE, results = 'asis'}
caption <- generate_figure_caption("Box-Cox Plot for Linear Model")
cat(caption)
par(mfrow=c(1,1))
bc <- boxcox(linear_model_initial, lambda=seq(-1,1,by=.1))
optimal_lambda <- bc$x[which.max(bc$y)]
print(optimal_lambda)
```

As we can observe from the box cox plot above, the lambda that give the highest log-likehood is `r optimal_lambda`. We transform Y using the log transformation as shown below. We will examine these regression assumptions below.

### Transformation Model

```{r transformation_model}
# Function to transform a single value back to the original scale
inverse_transform <- function(value, lambda=optimal_lambda) {
  if (lambda != 0) {
    return((value * lambda + 1)^(1 / lambda))
  } else {
    return(exp(value))
  }
}

# Function to calculate SSE, RMSE, MAE, and SST with or without inverse transformation
calculate_metrics <- function(actual, predicted, lambda = optimal_lambda) {
  if (lambda != 0) {
    # Apply inverse transformation to actual and predicted values
    actual_original <- inverse_transform(actual, lambda)
    predicted_original <- inverse_transform(predicted, lambda)

    # Calculate SSE, RMSE, MAE, and SST
    sse <- sum((actual_original - predicted_original)^2)
    rmse <- sqrt(sse / length(actual_original))
    mae <- mean(abs(actual_original - predicted_original))
    sst <- sum((actual_original - mean(actual_original))^2)
  } else {
    # Calculate SSE, RMSE, MAE, and SST without transformation
    sse <- sum((actual - predicted)^2)
    rmse <- sqrt(sse / length(actual))
    mae <- mean(abs(actual - predicted))
    sst <- sum((actual - mean(actual))^2)
  }

  return(list(sse = sse, rmse = rmse, mae = mae, sst = sst))
}

# Apply the Box-Cox transformation to the training and test data
if (optimal_lambda != 0) {
  transformed_price <- (train_df_linear$price^optimal_lambda - 1) / optimal_lambda
  transformed_test_price <- (test_df_linear$price^optimal_lambda - 1) / optimal_lambda
} else {
  transformed_price <- log(train_df_linear$price)
  transformed_test_price <- log(test_df_linear$price)
}

# Create a linear regression model using the transformed price
transformed_model <- lm(transformed_price ~ ., data = dplyr::select(train_df_linear, -price))

summary(transformed_model)

# Predict prices for training and test data using the transformed model
predicted_train_transformed <- predict(transformed_model, newdata = train_df_linear)
predicted_test_transformed <- predict(transformed_model, newdata = test_df_linear)

# Calculate other metrics for training and test data
metrics_train <- calculate_metrics(transformed_price, predicted_train_transformed, optimal_lambda)
metrics_test <- calculate_metrics(transformed_test_price, predicted_test_transformed, optimal_lambda)

# Calculate SSE for the training and test data without inverse transformation
sse_train <- sum((transformed_price - predicted_train_transformed)^2)
sse_test <- sum((transformed_test_price - predicted_test_transformed)^2)

# Calculate R-squared for training and test data without inverse transformation
sst_train <- sum((transformed_price - mean(transformed_price))^2)
sst_test <- sum((transformed_test_price - mean(transformed_test_price))^2)

r_squared_train <- 1 - (sse_train / sst_train)
r_squared_test <- 1 - (sse_test / sst_test)

# Create a dataframe to store the results
df_results <- rbind(df_results, data.frame(
  Model = "Transformed Model",
  SSE_train = metrics_train$sse,
  SSE_test = metrics_test$sse,
  R_squared_train = r_squared_train,
  R_squared_test = r_squared_test,
  RMSE_train = metrics_train$rmse,
  RMSE_test = metrics_test$rmse,
  MAE_train = metrics_train$mae,
  MAE_test = metrics_test$mae
))

# View the updated model results
view_model_results(df_results, caption = "Transformation Model Addition")
```

```{r transformed_model_plots, echo = FALSE, include = TRUE, results = 'asis'}
# Create a figure caption using the custom function
caption <- generate_figure_caption("Diagnostic Plots for Transformed Model")

coeff_trans <- transformed_model$coefficients

# Set the layout to 2x2
par(mfrow = c(2, 2))

# Plot the transformed_model and add the caption
cat(caption)
plot(transformed_model)
```



#### Columns with High VIF Values

```{r output_high_vif_values}
# Filter VIF values above the threshold
filtered_vif_values <- vif_values[vif_values >= vif_threshold]

# Create a data frame with the column names and filtered VIF values
vif_data <- data.frame(
  "Column Name" = names(filtered_vif_values),
  "VIF Value" = filtered_vif_values
)

# Create a figure caption using the custom function
caption <- generate_figure_caption("Variable Inflation Factors (VIF) Table (Above Threshold)")

# Generate the kable table without row names
kable(vif_data, format = "html", caption = caption, row.names = FALSE)
```

In our linear regression analysis, we have identified several columns with high Variance Inflation Factor (VIF) values exceeding the threshold of 10. VIF is a measure that quantifies the degree of multicollinearity between predictor variables in a regression model. High VIF values indicate that certain predictor variables are highly correlated with others, leading to multicollinearity issues. To build a more reliable and interpretable linear regression model, we are dropping these columns with high VIF values. By doing so, we aim to reduce the multicollinearity that can lead to unstable coefficient estimates and make it challenging to interpret the individual impact of each predictor variable on the target variable (price).

Despite the removal of these high VIF columns, it's worth noting that the model's performance has decreased slightly. The adjusted model, referred to as "OLS_linear_VIF_10," now exhibits a higher Sum of Squared Errors (SSE) for both the training and test datasets, indicating a decrease in predictive accuracy compared to the initial model. The R-squared values have also decreased slightly for both the training and test datasets. While the removal of highly correlated predictors was intended to improve model performance, it appears that in this case, it has led to a reduction in predictive accuracy. This outcome highlights the complexity of model building and the need for careful consideration of which variables to retain or remove to achieve the best model performance. Despite the decrease in performance, the adjusted model may still offer advantages in terms of interpretability and reduced multicollinearity.



#### Insights on WLS Linear Model

- The WLS Linear Model demonstrates a moderate \( R^2 \) for the training data (0.44367) and a higher \( R^2 \) for the test data (0.70391), suggesting better generalization than the Transformed Model.
- The RMSE and MAE values are higher compared to the basic OLS model, which could indicate a trade-off for achieving more robustness against heteroscedasticity.
- The significant difference in SSE between training and test datasets might suggest the need for further model tuning or exploration of alternative modeling approaches.

#### Conclusion

The WLS Linear Model provides a different perspective in handling data with heteroscedasticity. While it doesn't outperform the basic OLS model in terms of error metrics, it offers a more stable and generalizable model for the test dataset. This analysis underscores the importance of considering various models and transformations based on the specific characteristics of the dataset and the problem at hand.

## IV. Model Performance Testing
Use the test data set to assess the model performances. Here, build the best multiple linear models by using the stepwise both ways selection method. Compare the performance of the best two linear models. Make sure that model assumption(s) are checked for the final linear model. Apply remedy measures (transformation, etc.) that helps satisfy the assumptions. In particular, you must deeply investigate unequal variances and multicollinearity. If necessary, apply remedial methods (WLS, Ridge, Elastic Net, Lasso, etc.).

### OLS Stepwise Model

```{r step_models, echo = TRUE}
# Conditional logic based on the update_model_parameters flag
if (update_model_parameters) {
  # Run each of the stepwise regression models and update the JSON file
  # Step model both
  model_both <- ols_step_both_p(linear_model_initial, pent=0.35, prem=0.05)
  features_both <- setdiff(names(coef(model_both$model)), "(Intercept)")
  update_model_json("OLS_Step_Both", features_both, json_filepath)

  # Step model Backward
  model_backward <- ols_step_backward_p(linear_model_initial, pent=0.35, prem = 0.05)
  features_backward <- setdiff(names(coef(model_backward$model)), "(Intercept)")
  update_model_json("OLS_Step_Backward", features_backward, json_filepath)

  # Step model foward
  model_forward <- ols_step_forward_p(linear_model_initial, pent=0.35, penter = 0.05)
  features_forward <- setdiff(names(coef(model_forward$model)), "(Intercept)")
  update_model_json("OLS_Step_Forward", features_forward, json_filepath)
} else {
  # Load model parameters from JSON and build models
  model_params <- fromJSON(json_filepath)

  # Create models based on the loaded features
  if (all(c("OLS_Step_Both", "OLS_Step_Backward", "OLS_Step_Forward") %in% names(model_params))) {
    # For each model type, create the model using the features stored in the JSON
    model_both <- create_model(train_df_linear, 'price', model_params$OLS_Step_Both)
    model_backward <- create_model(train_df_linear, 'price', model_params$OLS_Step_Backward)
    model_forward <- create_model(train_df_linear, 'price', model_params$OLS_Step_Forward)
  } else {
    stop("Required model parameters are missing in the JSON file.")
  }
}

# Evaluate OLS_Step_Both
df_results <- evaluate_model("OLS_Step_Both", model_both, train_df_linear, test_df_linear, target_var = 'price', df_results)

# Evaluate OLS_Step_Backward
df_results <- evaluate_model("OLS_Step_Backward", model_backward, train_df_linear, test_df_linear, target_var = 'price', df_results)

# Evaluate OLS_Step_Forward
df_results <- evaluate_model("OLS_Step_Forward", model_forward, train_df_linear, test_df_linear, target_var = 'price', df_results)


view_model_results(df_results, "Step Model Additions")
```

### OLS Stepwise Model Outliers Removed

```{r refit_all_OLS_models, echo = TRUE}
# Fit a linear regression model to the training data
linear_model_initial_outliers <- lm(price ~ ., data = train_df_linear_outliers)

# Evaluate the initial linear model
df_results <- evaluate_model("OLS_Linear_outliers", linear_model_initial_outliers, train_df_linear_outliers, test_df_linear, target_var = 'price', df_results = df_results)

# Conditional logic based on the update_model_parameters flag
if (update_model_parameters) {
  # Run each of the stepwise regression models and update the JSON file
  # Step model both
  model_both_outliers <- ols_step_both_p(linear_model_initial_outliers, pent=0.35, prem=0.05)
  features_both <- setdiff(names(coef(model_both_outliers$model)), "(Intercept)")
  update_model_json("OLS_Step_Both_outliers", features_both, json_filepath)

  # Step model Backward
  model_backward_outliers <- ols_step_backward_p(linear_model_initial_outliers, pent=0.35,prem = 0.05)
  features_backward <- setdiff(names(coef(model_backward_outliers$model)), "(Intercept)")
  update_model_json("OLS_Step_Backward_outliers", features_backward, json_filepath)

  # Step model forward
  model_forward_outliers <- ols_step_forward_p(linear_model_initial_outliers, pent=0.35, penter = 0.05)
  features_forward <- setdiff(names(coef(model_forward_outliers$model)), "(Intercept)")
  update_model_json("OLS_Step_Forward_outliers", features_forward, json_filepath)
} else {
  # Load model parameters from JSON and build models
  model_params <- fromJSON(json_filepath)

  # Create models based on the loaded features
  if (all(c("OLS_Step_Both_outliers", "OLS_Step_Backward_outliers") %in% names(model_params))) {
    # For each model type, create the model using the features stored in the JSON
    # Create OLS Both model
    model_both_outliers <- create_model(train_df_linear_outliers, 'price', model_params$OLS_Step_Both_outliers)
    # Create OLS Backward model
    model_backward_outliers <- create_model(train_df_linear_outliers, 'price', model_params$OLS_Step_Backward_outliers)
    # Create OLS Forward model
    model_forward_outliers <- create_model(train_df_linear_outliers, 'price', model_params$OLS_Step_Forward_outliers)

  } else {
    stop("Required model parameters are missing in the JSON file.")
  }
}

# Evaluate the Both OLS reg
df_results <- evaluate_model("OLS_Step_Both_outliers", model_both_outliers, train_df_linear_outliers, test_df_linear, target_var = 'price', df_results)

# Evaluate the Backward OLS reg
df_results <- evaluate_model("OLS_Step_Backward_outliers", model_backward_outliers, train_df_linear_outliers, test_df_linear, target_var = 'price', df_results)

# Evalute the Forward OLS reg
df_results <- evaluate_model("OLS_Step_Forward_outliers", model_forward_outliers, train_df_linear_outliers, test_df_linear, target_var = 'price', df_results)

# Show initial linear model results
summary(linear_model_initial_outliers)

view_model_results(df_results, "Step Models without Outliers")
```

## V. Challenger Models
Build an alternative model based on one of the following approaches to predict price: regression tree, NN, or SVM. Explore using a logistic regression. Check the applicable model assumptions. Apply in-sample and out-of-sample testing, backtesting and review the comparative goodness of fit of the candidate models. Describe step by step your procedure to get to the best model and why you believe it is fit for purpose.


### Neural Network Model

```{r keras_NN}
# Define a normalization function
normalize_data <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

# Function to prepare the dataset
prepare_dataset <- function(df) {
  df$waterfront <- as.numeric(df$waterfront_1)
  selected_columns <- setdiff(names(df), c("season", "price_cat", "waterfront_0", "waterfront_1"))
  df_normalized <- as.data.frame(lapply(df[selected_columns], normalize_data))
  return(df_normalized)
}

# Function to rescale normalized data back to original scale
rescale_to_original <- function(x, min_val, max_val) {
  return((x * (max_val - min_val)) + min_val)
}

# Define model builder
build_model <- function(hp) {
  model <- keras_model_sequential() %>%
    layer_dense(units = hp$Int('units_1', min_value = 32, max_value = 128, step = 32),
                activation = 'relu', input_shape = ncol(train_data) - 1) %>%
    layer_dense(units = hp$Int('units_2', min_value = 16, max_value = 64, step = 16),
                activation = 'relu') %>%
    # Add more layers if needed
    layer_dense(units = 1)

  model %>% compile(
    optimizer = optimizer_adam(hp$Float('learning_rate', 1e-4, 1e-2, sampling = 'log')),
    loss = 'mean_squared_error',
    metrics = c('mean_absolute_error')
  )
  return(model)
}

train_and_evaluate_keras_nn <- function(train_data, test_data, original_train_price, original_test_price) {
  # Initialize variables to store the best model and its lowest validation MAE
  best_model <- NULL
  lowest_val_mae <- Inf
  best_batch_size <- NULL  # Store the best batch size
  best_epochs <- NULL  # Store the best number of epochs
  # Smaller batch sizes drastically increase computation required
  batch_sizes <- c(50, 60, 70, 80, 90, 100, 110, 120, 130)  # Different batch sizes to experiment with
  epochs_list <- c(50, 100, 150, 200, 250)  # Different numbers of epochs to experiment with

  for (batch_size in batch_sizes) {
    for (epochs in epochs_list) {
      # Define a complex Keras model with multiple layers
      model <- keras_model_sequential() %>%
        layer_dense(units = 256, activation = 'relu', input_shape = ncol(train_data) - 1) %>%
        layer_dropout(rate = 0.3) %>% # Accounts for multicollinearity and prevents overfit
        layer_dense(units = 128, activation = 'relu') %>%
        layer_dropout(rate = 0.3) %>%
        layer_dense(units = 64, activation = 'relu') %>%
        layer_dense(units = 32, activation = 'relu') %>%
        layer_dense(units = 16, activation = 'relu') %>%
        layer_dense(units = 8, activation = 'relu') %>%
        layer_dense(units = 4, activation = 'relu') %>%
        layer_dense(units = 2, activation = 'relu') %>%
        layer_dense(units = 1)

      # Compile the model
      model %>% compile(
        loss = 'mean_squared_error',
        optimizer = 'adam',
        metrics = c('mean_absolute_error')
      )

      # Fit the model to training data
      history <- model %>% fit(
        x = as.matrix(train_data[, -which(names(train_data) == "price")]),
        y = train_data$price,
        epochs = epochs,
        batch_size = batch_size,
        validation_split = 0.2  # Use 20% of training data for validation
      )

      # Check the performance on the validation set
      val_mae <- min(history$metrics$val_mean_absolute_error)
      if (val_mae < lowest_val_mae) {
        lowest_val_mae <- val_mae
        best_model <- model
        best_batch_size <- batch_size
        best_epochs <- epochs
      }
    }
  }

  # Print the best hyperparameters and their correlation
  cat("Best Batch Size:", best_batch_size, "\n")
  cat("Best Number of Epochs:", best_epochs, "\n")

  # Make predictions on the training and test data
  train_predictions <- predict(best_model, x = as.matrix(train_data[, -which(names(train_data) == "price")]))
  test_predictions <- predict(best_model, x = as.matrix(test_data[, -which(names(test_data) == "price")]))

  # Rescale predictions back to original scale
  rescaled_train_predictions <- rescale_to_original(train_predictions, min(original_train_price), max(original_train_price))
  rescaled_test_predictions <- rescale_to_original(test_predictions, min(original_test_price), max(original_test_price))

  # Calculate various performance metrics
  metrics <- list(
    mae_train = mean(abs(rescaled_train_predictions - original_train_price)),
    mae_test = mean(abs(rescaled_test_predictions - original_test_price)),
    RMSE_train = sqrt(mean((rescaled_train_predictions - original_train_price)^2)),
    RMSE_test = sqrt(mean((rescaled_test_predictions - original_test_price)^2)),
    SSE_train = sum((rescaled_train_predictions - original_train_price)^2),
    SSE_test = sum((rescaled_test_predictions - original_test_price)^2),
    correlation_train = cor(rescaled_train_predictions, original_train_price),
    correlation_test = cor(rescaled_test_predictions, original_test_price),
    r_squared_train = cor(rescaled_train_predictions, original_train_price)^2,
    r_squared_test = cor(rescaled_test_predictions, original_test_price)^2
  )

  print(paste("Model Correlation:", metrics$correlation_test))

  return(metrics)
}


# Load and prepare data
train_df_logis <- train_df_non_linear
test_df_logis <- test_df_non_linear

# Combine train and test data for normalization
combined_df <- rbind(train_df_logis, test_df_logis)

# Normalize the combined data
combined_norm <- prepare_dataset(combined_df)

# Split the normalized data back into train and test sets
train_norm <- combined_norm[1:nrow(train_df_logis), ]
test_norm <- combined_norm[(nrow(train_df_logis) + 1):nrow(combined_norm), ]

# Original non-normalized price data
original_train_price <- train_df_non_linear$price
original_test_price <- test_df_non_linear$price

# Train and evaluate the model
if (update_model_parameters) {
  library(keras)
  library(reticulate)
  # Import python as the backend for using Keras
  use_python(python = "C:\\Users\\Charl\\AppData\\Local\\Programs\\Python\\Python39\\python.exe", required = TRUE)
  # Fetch metrics
  nn_metrics <- train_and_evaluate_keras_nn(train_norm, test_norm, original_train_price, original_test_price)
  # Code to update model parameters in JSON
  update_model_json("NeuralNetwork", nn_metrics, json_filepath)
} else {
  # Code to load model parameters from JSON
  model_params <- fromJSON(json_filepath)
  nn_metrics <- model_params$NeuralNetwork
}

# Prepare results for the dataframe
nn_model_row <- data.frame(
  Model = "Neural Network",
  SSE_train = nn_metrics$SSE_train,
  SSE_test = nn_metrics$SSE_test,
  R_squared_train = nn_metrics$r_squared_train,
  R_squared_test = nn_metrics$r_squared_test,
  RMSE_train = nn_metrics$RMSE_train,
  RMSE_test = nn_metrics$RMSE_test,
  MAE_train = nn_metrics$mae_train,
  MAE_test = nn_metrics$mae_test,
  stringsAsFactors = FALSE
)

# Append the new row to df_results
df_results <- rbind(df_results, nn_model_row)
view_model_results(df_results, "Neural Network Model Addition")
```

### Regression Tree

```{r regression_tree}
# Create data frames for regression tree analysis
# Remove 'price_cat' and keep 'price' in the datasets
train_df_reg <- train_df_logis[, !names(train_df_logis) %in% "price_cat"]
test_df_reg <- test_df_logis[, !names(test_df_logis) %in% "price_cat"]

# Define a broader range of hyperparameters to search
tuneGrid <- expand.grid(
  cp = seq(0.001, 0.1, by = 0.001)
)

# Create a custom tuning grid for rpart
customControl <- trainControl(
  method = "cv",
  number = 5,
  search = "grid",
  verboseIter = FALSE
)

# Perform hyperparameter tuning with custom control
model <- train(
  price ~ .,
  data = train_df_reg,
  method = "rpart",
  trControl = customControl,
  tuneGrid = tuneGrid
)

# Make predictions
p.rpart_train <- predict(model, newdata = train_df_reg)
p.rpart_test <- predict(model, newdata = test_df_reg)

# Calculate metrics for training dataset
MAE_train <- mae(train_df_reg$price, p.rpart_train)
SSE_train <- sum((train_df_reg$price - p.rpart_train)^2)
R_squared_train <- R2(train_df_reg$price, p.rpart_train)
RMSE_train <- rmse(train_df_reg$price, p.rpart_train)

# Calculate metrics for testing dataset
MAE_test <- mae(test_df_reg$price, p.rpart_test)
SSE_test <- sum((test_df_reg$price - p.rpart_test)^2)
R_squared_test <- R2(test_df_reg$price, p.rpart_test)
RMSE_test <- rmse(test_df_reg$price, p.rpart_test)

# Append the results to df_results
df_results <- rbind(df_results, data.frame(
  Model = "Regression Tree",
  SSE_train = SSE_train,
  SSE_test = SSE_test,
  R_squared_train = R_squared_train,
  R_squared_test = R_squared_test,
  RMSE_train = RMSE_train,
  RMSE_test = RMSE_test,
  MAE_train = MAE_train,
  MAE_test = MAE_test,
  stringsAsFactors = FALSE
))

# View Model Results
view_model_results(df_results, "Regression Tree Model Addition")
```

### Logistic Regression

```{r}
# # Transform zipcode predictors
# indx.zipcode <- grepl('zipcode', colnames(train_df_non_linear_outliers))
# colname.zipcode <- colnames(train_df_non_linear_outliers)[indx.zipcode]
# train_df_logis <- train_df_non_linear_outliers
# test_df_logis <- test_df_non_linear
#
# # Function to calculate mean price for a given zipcode
# calc.mean.price <- function(col, df) {
#   df.select <- df[df[,col] == 1,]
#   mean.price <- mean(df.select$price)
#   return (mean.price)
# }
#
# # Initialize zipcode.meanprice column
# train_df_logis$zipcode.meanprice <- 0.0
# test_df_logis$zipcode.meanprice <- 0.0
#
# # Calculate zipcode.meanprice for each zipcode
# for (z in colname.zipcode) {
#   print (z)
#   train_df_logis[train_df_logis[,z] == 1,]$zipcode.meanprice <- calc.mean.price(z, train_df_logis)
#   test_df_logis[test_df_logis[,z] == 1,]$zipcode.meanprice <- calc.mean.price(z, test_df_logis)
# }
#
# # Retain all features except 'lat', 'long', and 'zipcode'
# train_df_logis <- train_df_logis[, !indx.zipcode]
# train_df_logis <- train_df_logis[, !names(train_df_logis) %in% c("lat", "long")]
# test_df_logis <- test_df_logis[, !indx.zipcode]
# test_df_logis <- test_df_logis[, !names(test_df_logis) %in% c("lat", "long")]
#
# # Create categorical response variable based on median price
# price_med <- median(train_df_logis$price)
# train_df_logis$price_cat <- ifelse(train_df_logis$price < price_med, 0, 1)
# train_df_logis$price_cat <- as.factor(train_df_logis$price_cat)
#
# price_med_test <- median(test_df_logis$price)
# test_df_logis$price_cat <- ifelse(test_df_logis$price < price_med_test, 0, 1)
# test_df_logis$price_cat <- as.factor(test_df_logis$price_cat)
#
# # logistic regression
# lmod <- glm(price_cat ~ . -price, family=binomial, train_df_logis)
# summary(lmod)
# # beta <- coef(lmod)
# # exp(beta)
#
# lmodr <- step(lmod, trace=0)
# summary(lmodr)
# length(summary(lmodr)$coefficients[,4])
#
# # drop1
# drop1(lmodr,test="Chi")
# length(summary(lmodr)$coefficients[,4])
# #dropping view_1 first (highest pvalue, 0.1255369 in Chi test)
# lmodr1<-update(lmodr, as.formula(paste(".~.-", "view_1")) )
# summary(lmodr1)
# length(summary(lmodr1)$coefficients[,4])
# drop1(lmodr1,test="Chi")
# #dropping view_2 next (highest pvalue 0.2810319 in Chi test)
# lmodr2<-update(lmodr1, as.formula(paste(".~.-", "view_2")) )
# summary(lmodr2)
# length(summary(lmodr2)$coefficients[,4])
# drop1(lmodr2,test="Chi")
# # now all variables are significant Chi test.
#
#
# #dropping view_1 next (highest pvalue, 0.983187 in summary)
# lmodr3<-update(lmodr2, as.formula(paste(".~.-", "grade_4")) )
# summary(lmodr3)
# length(summary(lmodr3)$coefficients[,4]) # now All variables are significant.
# #dropping grade_11 next (highest pvalue, 0.930191 in summary)
# lmodr4<-update(lmodr3, as.formula(paste(".~.-", "grade_11")) )
# summary(lmodr4)
# length(summary(lmodr4)$coefficients[,4]) # now All variables are significant.
# #dropping season next (highest pvalue, Fall 0.853800 in summary)
# lmodr5<-update(lmodr4, as.formula(paste(".~.-", "season")) )
# summary(lmodr5)
# length(summary(lmodr5)$coefficients[,4]) # now All variables are significant.
#
# lmodr.final <- lmodr5
#
# true_labels <- train_df_logis$price_cat
# exp<-cbind(true_labels,lmodr.final$fitted.values)
# exp[1:10,]
#
# #using 0.45 as a cutoff to predict the class. It has the highest F1
# predicted_probabilities <- predict(lmodr.final, train_df_logis, type="response")
# predictions <- ifelse(predicted_probabilities > 0.45, 1, 0)
# # Create confusion matrix
# confusion_matrix <- caret::confusionMatrix(as.factor(predictions),
#                                     as.factor(true_labels),
#                                     mode="prec_recall", positive = "1")
#
# confusion_matrix
#
# # # psuedo r squared
# # y.train <- as.numeric(train_df_logis$price_cat)
# # yhat.train <- as.numeric(predict(lmodr3, train_df_logis, type="response"))
# # sst.train <- sum((y.train - mean(y.train))^2)
# # sse.train <- sum((y.train - yhat.train)^2)
# # rsq.train <- 1 - sse.train / sst.train
# # rsq.train
#
# # test data
# true_labels.test <- test_df_logis$price_cat
# predicted_probabilities.test <- predict(lmodr.final, test_df_logis, type="response")
# predictions.test <- ifelse(predicted_probabilities.test > 0.45, 1, 0)
# # Create confusion matrix
# confusion_matrix.test <- caret::confusionMatrix(as.factor(predictions.test),
#                                     as.factor(true_labels.test),
#                                     mode="prec_recall", positive = "1")
#
# confusion_matrix.test
#
# confusion_matrix.output <- as.data.frame(rbind(c("train",round(confusion_matrix$byClass[[1]],3),
#                                     round(confusion_matrix$byClass[[2]],3),
#                                            round(confusion_matrix$byClass[[5]],3),
#                                            round(confusion_matrix$byClass[[7]],3)),
#                                          c("test",round(confusion_matrix.test$byClass[[1]],3),
#                                            round(confusion_matrix.test$byClass[[2]],3),
#                                            round(confusion_matrix.test$byClass[[5]],3),
#                                            round(confusion_matrix.test$byClass[[7]],3))) )
# colnames(confusion_matrix.output) <- c("data","Sensitivity","Specificity","Precision","F1")
# confusion_matrix.output
```

```{r logistic_regression}
# update_model_parameters <- TRUE
#
# if (update_model_parameters) {
#     # Calculate mean price for each zipcode
#     calc.mean.price <- function(col, df) {
#       df.select <- df[df[,col] == 1,]
#       mean.price <- mean(df.select$price)
#       return(mean.price)
#     }
#
#     # Adding mean price for each zipcode to the datasets
#     indx.zipcode <- grepl('zipcode', colnames(train_df_logis))
#     colname.zipcode <- colnames(train_df_logis)[indx.zipcode]
#     train_df_logis$zipcode.meanprice <- 0.0
#     test_df_logis$zipcode.meanprice <- 0.0
#
#     for (z in colname.zipcode) {
#       mean_price_train <- calc.mean.price(z, train_df_logis)
#       mean_price_test <- calc.mean.price(z, test_df_logis)
#
#       train_rows_to_update <- train_df_logis[, z] == 1
#       test_rows_to_update <- test_df_logis[, z] == 1
#
#       if (any(train_rows_to_update)) {
#         train_df_logis$zipcode.meanprice[train_rows_to_update] <- mean_price_train
#       } else {
#         warning(paste("No rows to update for train data in column:", z))
#       }
#
#       if (any(test_rows_to_update)) {
#         test_df_logis$zipcode.meanprice[test_rows_to_update] <- mean_price_test
#       } else {
#         warning(paste("No rows to update for test data in column:", z))
#       }
#       # Check the number of rows in train_df_logis
#       if (nrow(train_df_logis) == 0) {
#         stop("train_df_logis became empty after adding mean prices.")
#       }
#     }
#
#     # Removing unnecessary columns
#     train_df_logis <- train_df_logis[, !indx.zipcode]
#     train_df_logis <- train_df_logis[ , -which(names(train_df_logis) %in% c("lat", "long"))]
#
#     # Check the number of rows in train_df_logis
#     if (nrow(train_df_logis) == 0) {
#       stop("train_df_logis became empty after removing unnecessary columns.")
#     }
#
#     test_df_logis <- test_df_logis[, !indx.zipcode]
#     test_df_logis <- test_df_logis[ , -which(names(test_df_logis) %in% c("lat", "long"))]
#
#     outliers <- c("13244", "3098", "9986", "5127", "2627", "8814", "643")
#     # Handling Outliers
#     if (any(rownames(train_df_logis) %in% outliers)) {
#         train_df_logis <- train_df_logis[!rownames(train_df_logis) %in% outliers,]
#     } else {
#         warning("No outliers found or row names do not match in train_df_logis.")
#     }
#
#     # Check the number of rows in train_df_logis
#     if (nrow(train_df_logis) == 0) {
#       stop("train_df_logis became empty after handling outliers.")
#     }
#
#     # Creating a categorical response variable based on median price
#     price_med <- median(train_df_logis$price)
#     if (nrow(train_df_logis) > 0) {
#       train_df_logis$price_cat <- as.factor(ifelse(train_df_logis$price < price_med, 0, 1))
#     } else {
#       stop("train_df_logis is empty. Cannot create price_cat.")
#     }
#
#
#     if (nrow(test_df_logis) > 0) {
#       test_df_logis$price_cat <- as.factor(ifelse(test_df_logis$price < price_med.test, 0, 1))
#     } else {
#       stop("test_df_logis is empty. Cannot create price_cat.")
#     }
#
#     # Fit a logistic regression model
#     lmod <- glm(price_cat ~ . -price, family = binomial, data = train_df_logis)
#
#     # Perform stepwise model selection to drop non-significant variables
#     lmodr <- step(lmod, trace = 0)
#
#     # Use the 'drop1' function to assess variable importance and drop non-significant variables
#     # Continue this process until all variables are significant
#     while(any(drop1(lmodr, test = "Chi")$`Pr(>Chi)` > 0.05)) {
#       variable_to_drop <- names(which.max(drop1(lmodr, test = "Chi")$`Pr(>Chi)`))
#       lmodr <- update(lmodr, as.formula(paste(".~.-", variable_to_drop)))
#     }
#
#     # Use the final model
#     lmodr.final <- lmodr
#
#     # Predict probabilities and classify based on the threshold
#     predicted_probabilities <- predict(lmodr.final, train_df_logis, type = "response")
#     predictions <- ifelse(predicted_probabilities > 0.45, 1, 0)
#
#     # Create a confusion matrix for training data
#     confusion_matrix <- confusionMatrix(as.factor(predictions), as.factor(train_df_logis$price_cat), mode = "prec_recall", positive = "1")
#
#     # Extract relevant metrics from the confusion matrix
#     sensitivity <- confusion_matrix$byClass['Sensitivity']
#     specificity <- confusion_matrix$byClass['Specificity']
#     precision <- confusion_matrix$byClass['Precision']
#     F1 <- confusion_matrix$byClass['F1']
#
#     # Construct the list of model metrics for logistic regression
#     logistic_regression_metrics <- list(
#         sensitivity = sensitivity,
#         specificity = specificity,
#         precision = precision,
#         F1 = F1,
#         model_coefficients = summary(lmodr.final)$coefficients,
#         AIC = AIC(lmodr.final),
#         BIC = BIC(lmodr.final)
#     )
#
#     # Update the model parameters in the JSON file
#     update_model_json("LogisticRegression", logistic_regression_metrics, json_filepath)
#
# } else {
#     # Load model parameters from JSON
#     model_params <- fromJSON(json_filepath)
#     logistic_regression_metrics <- model_params$LogisticRegression
# }
#
#
# # Create a new dataframe for logistic regression results
# df_results_categorical <- data.frame(
#     Model = "Logistic Regression",
#     Sensitivity = logistic_regression_metrics$sensitivity,
#     Specificity = logistic_regression_metrics$specificity,
#     Precision = logistic_regression_metrics$precision,
#     F1 = logistic_regression_metrics$F1,
#     AIC = logistic_regression_metrics$AIC,
#     BIC = logistic_regression_metrics$BIC,
#     stringsAsFactors = FALSE
# )
#
# # Append coefficients to df_results_categorical if necessary
# coefficients_df <- as.data.frame(logistic_regression_metrics$model_coefficients)
# df_results_categorical <- cbind(df_results_categorical, coefficients_df)
#
# # Print the results dataframe
# view(df_results_categorical)
```

### Decision Tree

```{r decision_tree}
# # Create data frames for decision tree analysis
# train_df_dec <- train_df_logis
# test_df_dec <- test_df_logis
#
# # Create a categorical response variable: 1=higher than 200,000, 0=lower than 200,000
# train_df_dec$price_low <- 1
# train_df_dec[train_df_dec$price > 200000,]$price_low <- 0
# train_df_dec$price_low <- as.factor(train_df_dec$price_low)
# table(train_df_dec$price_low)
#
# test_df_dec$price_low <- 1
# test_df_dec[test_df_dec$price > 200000,]$price_low <- 0
# test_df_dec$price_low <- as.factor(test_df_dec$price_low)
# table(test_df_dec$price_low)
#
# # Remove 'price' and 'price_cat' columns
# train_df_dec <- train_df_dec[, -which(names(train_df_dec) %in% c("price", "price_cat"))]
# test_df_dec <- test_df_dec[, -which(names(test_df_dec) %in% c("price", "price_cat"))]
#
# # Load the 'C50' library for decision tree modeling
# library(C50)
#
# # Fit a decision tree model
# price_model <- C5.0(train_df_dec, train_df_dec$price_low)
# price_model
#
# # Evaluate the model's performance
# summary(price_model)
#
# # Visualize the decision tree
# plot(price_model)
#
# # Make predictions on the test data
# price_pred <- predict(price_model, test_df_dec)
#
# # Evaluate the model's performance using a confusion matrix
# library(gmodels)
# CrossTable(test_df_dec$price_low, price_pred,
#            prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
#            dnn = c('actual default', 'predicted default'))
#
# # Boosting
# price_boost10 <- C5.0(train_df_dec, train_df_dec$price_low, trials = 10)
# summary(price_boost10)
```

## VI. Model Limitation and Assumptions
Based on the performances on both train and test data sets, determine your primary (champion) model and the other model which would be your benchmark model. Validate your models using the test sample. Do the residuals look normal? Does it matter given your technique? How is the prediction performance using Pseudo R^2, SSE, RMSE?  Benchmark the model against alternatives. How good is the relative fit? Are there any serious violations of the model assumptions? Has the model had issues or limitations that the user must know? (Which assumptions are needed to support the Champion model?)


## VII. Ongoing Model Monitoring Plan
How would you picture the model needing to be monitored, which quantitative thresholds and triggers would you set to decide when the model needs to be replaced? What are the assumptions that the model must comply with for its continuous use?

## VIII. Conclusion
Summarize your results here. What is the best model for the data and why?


## Bibliography

https://datatofish.com/remove-column-dataframe-r/

https://www.statology.org/r-convert-date-to-numeric/

https://www.marsja.se/create-dummy-variables-in-r/

https://www.geeksforgeeks.org/how-to-find-and-count-missing-values-in-r-dataframe/

https://intro2r.com/export_plots.html

https://stackoverflow.com/questions/25166624/insert-picture-table-in-r-markdown

https://www.tutorialspoint.com/how-to-deal-with-error-error-in-shapiro-test-sample-size-must-be-between-3-and-5000-in-r#:~:text=Error%20in%20shapiro.-,test(%E2%80%A6)%20%3A%20sample%20size%20must%20be%20between,3%20and%205000%E2%80%9D%20in%20R%3F&text=The%20shapiro.,called%20Anderson%20Darling%20normality%20test.


Book and decks of the Data modelling class

Please include all references, articles and papers in this section.


## Appendix

Please add any additional supporting graphs, plots and data analysis.





